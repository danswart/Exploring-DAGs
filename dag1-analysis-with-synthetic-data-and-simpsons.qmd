---
title: "DAG Analysis with Synthetic Data - Fork Structure"
author: "Dan Swart and Claude 3.7 Sonnet"
format: 
  html:
    toc: true
    toc-float: true
    page-layout: article
    embed-resources: true
    code-fold: true
    code-summary: "Show the code"
    code-tools: true
    code-overflow: wrap
    code-block-bg: "#FAEBD7"
    code-block-border-left: "#31BAE9"
    code-link: true          # This adds individual buttons
    fig-width: 10
    fig-height: 8
    fig-align: center
    html-math-method: katex
    css: swart-20250327.css
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    papersize: letter
    geometry:
      - margin=1in
    fig-width: 10
    fig-height: 8
    fig-pos: 'H'
  typst:
    toc: true
    fig-width: 10
    fig-height: 8
    keep-tex: true
    prefer-html: true
---

```{r}
#| label: init-setup
#| include: false
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE)

# Load required packages
library(tidyverse)    # For dplyr, ggplot, and friends
library(ggdag)        # For plotting DAGs
library(dagitty)      # For working with DAG logic
library(DiagrammeR)   # For complete control of the layout
library(knitr)        # For controlling rendering
library(kableExtra)   # For tables summarizing results
library(DT)           # For rendering interactive tables
library(rethinking)   # For causal inference models 
library(broom)        # For tidying model output
```

## 1. Introduction: Testing a Fork Structure DAG with Synthetic Data

This document demonstrates how to test a simple fork structure (confounder) DAG using synthetic data. In our DAG, we have:

- **X**: Exposure variable (e.g., coffee consumption)
- **Y**: Outcome variable (e.g., heart disease risk)
- **Z**: Confounder variable (e.g., age)

Our causal model specifies that:
- Z affects both X and Y (Z → X and Z → Y)
- X affects Y (X → Y)

This structure creates a classic confounding scenario where failing to account for Z will lead to biased estimates of the effect of X on Y.

## 2. Creating Synthetic Data

First, let's create synthetic data that follows our DAG structure. We'll set the true causal relationships with specific coefficients:

```{r}
#| label: create-synthetic-data
#| message: false
#| warning: false

# Set seed for reproducibility
set.seed(42)

# Sample size
n <- 1000

# Generate synthetic data following the DAG structure
# Z → X, Z → Y, and X → Y

# First, generate the confounder Z (e.g., age)
Z <- rnorm(n, mean = 50, sd = 10)

# Next, generate X as influenced by Z (e.g., coffee consumption affected by age)
# We add some random noise to represent other factors affecting X
X <- 2 + 0.08 * Z + rnorm(n, mean = 0, sd = 1)

# Finally, generate Y as influenced by both Z and X
# The true direct effect of X on Y is 0.3
# The true effect of Z on Y is 0.1
Y <- 5 + 0.3 * X + 0.1 * Z + rnorm(n, mean = 0, sd = 1.5)

# Create a data frame
dag_data <- data.frame(X = X, Y = Y, Z = Z)

# Quick summary of the data
summary(dag_data)
```

Our synthetic data now follows these true causal relationships:
- Z → X with coefficient 0.08 (1 unit increase in Z causes 0.08 unit increase in X)
- Z → Y with coefficient 0.1 (1 unit increase in Z causes 0.1 unit increase in Y)
- X → Y with coefficient 0.3 (1 unit increase in X causes 0.3 unit increase in Y)

Let's visualize the distributions and relationships in our synthetic data:

```{r}
#| label: visualize-data-distributions
#| fig-cap: "Distributions and relationships in our synthetic data"
#| fig-subcap: 
#|   - "Distributions of X, Y, and Z"
#|   - "Scatterplot of X vs Y"
#|   - "Scatterplot of Z vs X"
#|   - "Scatterplot of Z vs Y"
#| layout-ncol: 2

# Distributions of variables
dag_data %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value)) +
  geom_histogram(fill = "steelblue", alpha = 0.7, bins = 30) +
  facet_wrap(~ Variable, scales = "free") +
  theme_minimal() +
  ggtitle("Distributions of X, Y, and Z")

# X vs Y scatterplot
ggplot(dag_data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.3, color = "dodgerblue") +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  theme_minimal() +
  ggtitle("Relationship between X and Y")

# Z vs X scatterplot
ggplot(dag_data, aes(x = Z, y = X)) +
  geom_point(alpha = 0.3, color = "darkgreen") +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  theme_minimal() +
  ggtitle("Relationship between Z and X")

# Z vs Y scatterplot
ggplot(dag_data, aes(x = Z, y = Y)) +
  geom_point(alpha = 0.3, color = "purple") +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  theme_minimal() +
  ggtitle("Relationship between Z and Y")
```

## 3. Recreating the DAG for Reference

Let's recreate the DAG using DiagrammeR and ggdag for reference:

```{r}
#| label: visualize-dag-structure
#| message: false
#| warning: false
#| fig-cap: "DAG visualization showing the fork structure"
#| fig-subcap: 
#|   - "DiagrammeR Visualization"
#|   - "ggdag Visualization"
#| layout-ncol: 1

# Visualize DAG with DiagrammeR
grViz("
  digraph DAG {
    # Graph settings
    graph [layout=neato, margin=\"0.0, 0.0, 0.0, 0.0\"]
    
    # Add a title
    labelloc=\"t\"
    label=\"Causal Fork Structure DAG\\n   \"
    fontsize=16
    
    # Node settings
    node [shape=plaintext, fontsize=16, fontname=\"Arial\"]
    
    # Edge settings
    edge [penwidth=1.50, color=\"darkblue\", arrowsize=1.00]
    
    # Nodes with exact coordinates
    X [label=\"X (Exposure)\", pos=\"1.0, 1.0!\", fontcolor=\"dodgerblue\"]
    Y [label=\"Y (Outcome)\", pos=\"3.0, 1.0!\", fontcolor=\"dodgerblue\"]
    Z [label=\"Z (Confounder)\", pos=\"2.0,3.0!\", fontcolor=\"red\"]
    
    # Edges with true coefficients from our synthetic data
    X -> Y [label=\"0.3\"]
    Z -> X [label=\"0.08\"]
    Z -> Y [label=\"0.1\"]
    
    # Caption
    Caption [shape=plaintext, label=\"Figure 1: Z represents a classic confounding structure\", 
             fontsize=10, pos=\"2,0.0!\"]
  }
")

# Define the DAG using ggdag
dag <- dagify(
  Y ~ X + Z,
  X ~ Z,
  exposure = "X",
  outcome = "Y",
  labels = c("X" = "X (Exposure)", 
             "Y" = "Y (Outcome)", 
             "Z" = "Z (Confounder)")
)

# Set coordinates for nice visualization
coordinates(dag) <- list(
  x = c(X = 1, Y = 3, Z = 2),
  y = c(X = 1, Y = 1, Z = 3)
)

# Create nice visualization with ggdag
ggdag(dag, edge_type = "link") + 
  geom_dag_point(color = "lightblue", size = 14, alpha = 0.7) +
  geom_dag_text(color = "black") +
  geom_dag_edges(edge_colour = "blue", edge_width = 1.0, arrow_size = 0.6) +
  theme_dag() +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("DAG: X → Y with Z as confounder")
```

## 4. Testing Causal Relationships

Now let's test the causal relationships in our DAG by comparing models with and without adjusting for the confounder Z.

### 4.1 Unadjusted Model (Biased Estimate)

First, we'll fit a model that ignores the confounder Z:

```{r}
#| label: fit-unadjusted-model
#| message: false

# Fit unadjusted model (ignoring the confounder Z)
model_unadjusted <- lm(Y ~ X, data = dag_data)

# Display model summary
summary(model_unadjusted)

# Extract the coefficient for X
coef_unadjusted <- coef(model_unadjusted)["X"]
```

### 4.2 Adjusted Model (Correcting for Confounding)

Now, let's fit a model that adjusts for the confounder Z:

```{r}
#| label: fit-adjusted-model
#| message: false

# Fit adjusted model (accounting for confounder Z)
model_adjusted <- lm(Y ~ X + Z, data = dag_data)

# Display model summary
summary(model_adjusted)

# Extract the coefficient for X
coef_adjusted <- coef(model_adjusted)["X"]
```

### 4.3 Comparing Model Results

Let's compare the estimated effect of X on Y from both models:

```{r}
#| label: compare-model-results
#| message: false

# True effect of X on Y (from our data generation process)
true_effect <- 0.3

# Create a comparison table
comparison_df <- data.frame(
  Model = c("True Causal Effect", "Unadjusted Model (Ignores Z)", "Adjusted Model (Controls for Z)"),
  Coefficient = c(true_effect, coef_unadjusted, coef_adjusted),
  Error = c(0, coef_unadjusted - true_effect, coef_adjusted - true_effect),
  BiasPercent = c(0, 100 * (coef_unadjusted - true_effect) / true_effect, 
                  100 * (coef_adjusted - true_effect) / true_effect)
)

# Format for display
comparison_df$Coefficient <- round(comparison_df$Coefficient, 4)
comparison_df$Error <- round(comparison_df$Error, 4)
comparison_df$BiasPercent <- round(comparison_df$BiasPercent, 2)

# Display as a table
kable(comparison_df, 
      col.names = c("Model", "Coefficient of X on Y", "Absolute Error", "Bias (%)"),
      caption = "Comparison of estimated effects of X on Y") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

### 4.4 Testing for Significant Differences Between Models

Now, let's test whether the difference between the unadjusted and adjusted models is statistically significant, and whether our adjusted model recovers the true causal effect:

```{r}
#| label: test-model-significance
#| message: false

# Compare models using anova (valid since one model is nested in the other)
model_comparison <- anova(model_unadjusted, model_adjusted)

# Test if unadjusted coefficient differs from true effect
unadj_z_stat <- (coef_unadjusted - true_effect) / summary(model_unadjusted)$coefficients["X", "Std. Error"]
unadj_p_value <- 2 * (1 - pnorm(abs(unadj_z_stat)))

# Test if adjusted coefficient differs from true effect
adj_z_stat <- (coef_adjusted - true_effect) / summary(model_adjusted)$coefficients["X", "Std. Error"]
adj_p_value <- 2 * (1 - pnorm(abs(adj_z_stat)))

# Create a data frame for the results
significance_df <- data.frame(
  Comparison = c(
    "Unadjusted vs. Adjusted Model",
    "Unadjusted Model vs. True Effect",
    "Adjusted Model vs. True Effect"
  ),
  
  Test = c(
    "F-test (ANOVA)",
    "Z-test (coefficient vs. true effect)",
    "Z-test (coefficient vs. true effect)"
  ),
  
  Statistic = c(
    round(model_comparison$F[2], 3),
    round(unadj_z_stat, 3),
    round(adj_z_stat, 3)
  ),
  
  PValue = c(
    round(model_comparison$`Pr(>F)`[2], 4),
    round(unadj_p_value, 4),
    round(adj_p_value, 4)
  ),
  
  Conclusion = c(
    ifelse(model_comparison$`Pr(>F)`[2] < 0.05, 
           "Models are significantly different", 
           "No significant difference between models"),
    
    ifelse(unadj_p_value < 0.05,
           "Unadjusted estimate is significantly different from true effect",
           "Unadjusted estimate is not significantly different from true effect"),
    
    ifelse(adj_p_value < 0.05,
           "Adjusted estimate is significantly different from true effect",
           "Adjusted estimate is not significantly different from true effect")
  )
)

# Display as a table
kable(significance_df, 
      col.names = c("Comparison", "Test", "Statistic", "P-value", "Conclusion"),
      caption = "Statistical tests for model comparisons") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

These tests confirm that:

1. The unadjusted and adjusted models are significantly different from each other (ANOVA F-test)
2. The unadjusted model's estimate is significantly different from the true causal effect (confirming bias)
3. The adjusted model's estimate is not significantly different from the true causal effect (confirming it recovers the true relationship)

This statistical evidence supports our claim that failing to control for confounder Z leads to a significantly biased estimate of the effect of X on Y.

## 5. Testing Implied Conditional Independences 

The fork structure DAG implies a specific conditional independence: **X and Y should be conditionally independent given Z** if the only association between X and Y is through Z. However, since we have a direct effect of X on Y, we should see that X and Y remain associated even after conditioning on Z.

Let's test this using partial correlations:

```{r}
#| label: test-conditional-independence
#| message: false

# Function to calculate partial correlation
partial_cor <- function(x, y, z, data) {
  model_x <- lm(as.formula(paste(x, "~", z)), data = data)
  model_y <- lm(as.formula(paste(y, "~", z)), data = data)
  
  residuals_x <- residuals(model_x)
  residuals_y <- residuals(model_y)
  
  cor(residuals_x, residuals_y)
}

# Calculate simple correlation between X and Y
simple_cor_XY <- cor(dag_data$X, dag_data$Y)

# Calculate partial correlation between X and Y, controlling for Z
partial_cor_XY_Z <- partial_cor("X", "Y", "Z", dag_data)

# Prepare results for display
cor_tests <- data.frame(
  Test = c("Simple correlation between X and Y", 
           "Partial correlation between X and Y, controlling for Z"),
  Correlation = c(simple_cor_XY, partial_cor_XY_Z)
)

# Format for display
cor_tests$Correlation <- round(cor_tests$Correlation, 4)

# Display as a table
kable(cor_tests, 
      col.names = c("Test", "Correlation"),
      caption = "Testing conditional independences") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

The results confirm our understanding of the DAG structure:

1. The simple correlation between X and Y is substantial, reflecting both the direct effect of X on Y and the indirect association through Z
2. The partial correlation between X and Y controlling for Z is reduced but still significant, confirming the direct causal relationship between X and Y

## 6. Path Analysis in Our DAG

Our DAG has two paths from X to Y:
1. The direct path: X → Y (causal effect)
2. The backdoor path: X ← Z → Y (confounding)

Let's visualize these paths:

```{r}
#| label: visualize-dag-paths
#| message: false
#| warning: false
#| fig-cap: "Path analysis in our DAG"

# Use ggdag to visualize paths
ggdag_paths(dag, from = "X", to = "Y", shadow = TRUE) +
  theme_dag() +
  ggtitle("All paths from X to Y")
```

Now, let's use ggdag to highlight the appropriate adjustment set:

```{r}
#| label: visualize-adjustment-set
#| message: false
#| warning: false
#| fig-cap: "Adjustment set for estimating the causal effect of X on Y"

# Visualize the adjustment set
ggdag_adjustment_set(dag, exposure = "X", outcome = "Y", shadow = TRUE) +
  theme_dag() +
  ggtitle("Adjustment set to estimate causal effect of X on Y")
```

## 7. Examining the Backdoor Criterion

The backdoor criterion states that to identify the causal effect of X on Y, we need to block all backdoor paths between X and Y. In our case, there's one backdoor path: X ← Z → Y.

Let's see how controlling for Z changes the estimated relationship between X and Y by visualizing this process:

```{r}
#| label: visualize-backdoor-criterion
#| fig-cap: "Visualizing how controlling for Z removes confounding"
#| fig-subcap: 
#|   - "Relationship between X and Y (unadjusted)"
#|   - "Relationship between Z and X (confounder → exposure)"
#|   - "Relationship between Z and Y (confounder → outcome)"
#|   - "Relationship between X and Y after adjusting for Z"
#| layout-ncol: 2

# 1. X-Y relationship (unadjusted)
p1 <- ggplot(dag_data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.3, color = "dodgerblue") +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  theme_minimal() +
  annotate("text", x = min(dag_data$X) + 0.2*(max(dag_data$X)-min(dag_data$X)), 
           y = max(dag_data$Y) - 0.1*(max(dag_data$Y)-min(dag_data$Y)), 
           label = paste("Slope =", round(coef(lm(Y ~ X, dag_data))[2], 3)),
           hjust = 0) +
  ggtitle("Unadjusted X-Y relationship")

# 2. Z-X relationship (correctly labeled with Z as predictor)
p2 <- ggplot(dag_data, aes(x = Z, y = X)) +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  geom_point(alpha = 0.3, color = "darkgreen") +
  theme_minimal() +
  annotate("text", x = min(dag_data$Z) + 0.2*(max(dag_data$Z)-min(dag_data$Z)), 
           y = max(dag_data$X) - 0.1*(max(dag_data$X)-min(dag_data$X)), 
           label = paste("Slope =", round(coef(lm(X ~ Z, dag_data))[2], 3)),
           hjust = 0) +
  ggtitle("Z → X relationship (confounder affects exposure)")

# 3. Z-Y relationship (correctly labeled with Z as predictor)
p3 <- ggplot(dag_data, aes(x = Z, y = Y)) +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  geom_point(alpha = 0.3, color = "purple") +
  theme_minimal() +
  annotate("text", x = min(dag_data$Z) + 0.2*(max(dag_data$Z)-min(dag_data$Z)), 
           y = max(dag_data$Y) - 0.1*(max(dag_data$Y)-min(dag_data$Y)), 
           label = paste("Slope =", round(coef(lm(Y ~ Z, dag_data))[2], 3)),
           hjust = 0) +
  ggtitle("Z → Y relationship (confounder affects outcome)")

# 4. X-Y relationship (adjusted for Z) - using residuals
# Create residuals
x_resid <- residuals(lm(X ~ Z, data = dag_data))
y_resid <- residuals(lm(Y ~ Z, data = dag_data))
resid_data <- data.frame(x_resid = x_resid, y_resid = y_resid)

# Plot relationship between residuals
p4 <- ggplot(resid_data, aes(x = x_resid, y = y_resid)) +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  geom_point(alpha = 0.3, color = "orange") +
  theme_minimal() +
  annotate("text", x = min(resid_data$x_resid) + 0.2*(max(resid_data$x_resid)-min(resid_data$x_resid)), 
           y = max(resid_data$y_resid) - 0.1*(max(resid_data$y_resid)-min(resid_data$y_resid)), 
           label = paste("Slope =", round(coef(lm(y_resid ~ x_resid))[2], 3)),
           hjust = 0) +
  xlab("X (residuals after controlling for Z)") +
  ylab("Y (residuals after controlling for Z)") +
  ggtitle("X → Y relationship after removing Z's effect")

# Display all plots
p1
p2
p3
p4
```

The visualizations above demonstrate the process of controlling for a confounder:

1. The unadjusted X-Y relationship (top left) shows a strong association that includes both the causal effect and confounding
2. The Z-X relationship (top right) shows how the confounder affects our exposure variable
3. The Z-Y relationship (bottom left) shows how the confounder affects our outcome variable
4. The adjusted X-Y relationship (bottom right), after removing Z's effect from both X and Y, shows the true causal relationship

## 8. D-Separation Analysis

D-separation is a criterion for determining whether two variables in a DAG are conditionally independent given a set of other variables. Let's test the d-separation claims from our DAG:

```{r}
#| label: test-d-separation
#| message: false

# Test d-separation claims in our DAG
dseparation_results <- data.frame(
  Claim = c(
    "X and Y are not d-separated (without conditioning)",
    "X and Y are not d-separated given Z"
  ),
  Result = c(
    !dseparated(dag, "X", "Y", c()),
    !dseparated(dag, "X", "Y", c("Z"))
  ),
  Explanation = c(
    "X and Y are associated through both the direct path and the backdoor path",
    "Even after blocking the backdoor path, X and Y remain associated through the direct path"
  )
)

# Display as a table
kable(dseparation_results, 
      col.names = c("D-Separation Claim", "Confirmed?", "Explanation"),
      caption = "Testing d-separation claims in our DAG") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

These results confirm our understanding of the DAG structure:

1. X and Y are not d-separated (without conditioning) because they have both a direct connection and a backdoor path through Z
2. X and Y remain associated even after conditioning on Z because of the direct causal path from X to Y

## 9. Sensitivity Analysis: What if Z is Unmeasured?

In real-world scenarios, we might not be able to measure all confounders. Let's conduct a sensitivity analysis to understand the impact of unmeasured confounding:

```{r}
#| label: conduct-sensitivity-analysis
#| message: false
#| warning: false

# Function to simulate unmeasured confounding
simulate_unmeasured_z <- function(cor_xz, cor_zy, n = 1000) {
  # Create a new dataset without Z
  z_unmeasured <- rnorm(n)
  
  # Create X correlated with unmeasured Z
  x_unmeasured <- cor_xz * z_unmeasured + sqrt(1 - cor_xz^2) * rnorm(n)
  
  # Create Y correlated with unmeasured Z and with a direct effect from X (set to 0.3)
  y_unmeasured <- 0.3 * x_unmeasured + cor_zy * z_unmeasured + 
                  sqrt(1 - 0.3^2 - cor_zy^2 - 2*0.3*cor_xz*cor_zy) * rnorm(n)
  
  # Create dataset
  data.frame(X = x_unmeasured, Y = y_unmeasured)
}

# Create a range of confounding strengths
z_effects <- seq(0, 0.5, by = 0.1)
results <- data.frame()

# For each confounding strength, calculate the bias
for (z_effect in z_effects) {
  # Simulate data with unmeasured confounding
  sim_data <- simulate_unmeasured_z(cor_xz = z_effect, cor_zy = z_effect)
  
  # Fit model without ability to adjust for Z
  model <- lm(Y ~ X, data = sim_data)
  
  # Store results
  results <- rbind(results, data.frame(
    ConfounderStrength = z_effect,
    EstimatedEffect = coef(model)["X"],
    TrueEffect = 0.3,
    Bias = coef(model)["X"] - 0.3,
    BiasPercent = 100 * (coef(model)["X"] - 0.3) / 0.3
  ))
}

# Plot the results
ggplot(results, aes(x = ConfounderStrength, y = EstimatedEffect)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "blue", size = 3) +
  geom_hline(yintercept = 0.3, linetype = "dashed", color = "red") +
  labs(
    title = "Impact of Unmeasured Confounding on Effect Estimates",
    subtitle = "Red dashed line shows true causal effect (0.3)",
    x = "Confounder Strength (correlation between Z and X/Y)",
    y = "Estimated Effect of X on Y"
  ) +
  theme_minimal()
```

This sensitivity analysis shows how unmeasured confounding can bias our estimates of the causal effect. As the strength of confounding increases (stronger correlation between Z and both X and Y), our estimate of the effect of X on Y becomes increasingly biased.

## 10. Beyond Simple Regression: Causal Simulation and Estimation

Let's explore a more advanced approach using the `rethinking` package, which is especially good for causal inference:

```{r}
#| label: fit-bayesian-model
#| message: false
#| warning: false

# Standardize the variables for better fitting
dag_data_std <- dag_data %>%
  mutate(across(everything(), scale))

# Fit a Bayesian causal model for the adjusted effect
m_adjusted <- quap(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + bX * X + bZ * Z,
    a ~ dnorm(0, 1),
    bX ~ dnorm(0, 1),
    bZ ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = dag_data_std
)

# Extract the posterior distribution
post <- extract.samples(m_adjusted)

# Summarize the posterior for the causal effect of X on Y
causal_effect_summary <- data.frame(
  Parameter = "Effect of X on Y",
  Mean = mean(post$bX),
  SD = sd(post$bX),
  `2.5%` = quantile(post$bX, 0.025),
  `50%` = median(post$bX),
  `97.5%` = quantile(post$bX, 0.975)
)

# Display the causal effect estimate
kable(causal_effect_summary, 
      caption = "Bayesian estimate of the causal effect of X on Y (adjusted for Z)",
      digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Plot the posterior distribution of the causal effect
ggplot(data.frame(bX = post$bX), aes(x = bX)) +
  geom_density(fill = "skyblue", alpha = 0.7) +
  geom_vline(xintercept = mean(post$bX), color = "darkblue", linetype = "dashed") +
  labs(
    title = "Posterior Distribution of the Causal Effect of X on Y",
    subtitle = "After adjusting for confounder Z",
    x = "Effect Size (Standardized)",
    y = "Density"
  ) +
  theme_minimal()
```

## 11. Counterfactual Analysis: "What If" Scenarios

Causal inference allows us to answer counterfactual questions. Let's explore what would happen to Y if we intervened to change X, while holding Z constant:


```{r}
#| label: counterfactual-analysis
#| message: false
#| warning: false

# Function to predict Y based on do(X = x)
predict_counterfactual <- function(x_values, fixed_z = 50) {
  # Use the coefficients from our adjusted model
  intercept <- coef(model_adjusted)[1]
  x_coef <- coef(model_adjusted)[2]
  z_coef <- coef(model_adjusted)[3]
  
  # Predict Y for different values of X, holding Z constant
  y_pred <- intercept + x_coef * x_values + z_coef * fixed_z
  return(y_pred)
}

# Create a range of X values
x_range <- seq(min(dag_data$X), max(dag_data$X), length.out = 100)

# Predict Y for different values of X, holding Z constant at its mean
z_mean <- mean(dag_data$Z)
y_pred <- predict_counterfactual(x_range, fixed_z = z_mean)

# Create a data frame for plotting
counterfactual_df <- data.frame(X = x_range, Y = y_pred)

# Plot the counterfactual prediction
ggplot() +
  # Add actual data points
  geom_point(data = dag_data, aes(x = X, y = Y), alpha = 0.2, color = "gray") +
  # Add counterfactual prediction
  geom_line(data = counterfactual_df, aes(x = X, y = Y), 
            color = "red", size = 1.2) +
  labs(
    title = "Counterfactual Prediction: What if we change X?",
    subtitle = paste("Holding Z constant at its mean:", round(z_mean, 2)),
    x = "X (Exposure)",
    y = "Y (Outcome)"
  ) +
  theme_minimal()
```

The red line represents the causal effect of X on Y, isolated from confounding by holding Z constant. This is the relationship we would observe if we could experimentally manipulate X while keeping Z fixed.

To understand how the counterfactual prediction changes for different values of Z, let's create a set of predictions for various Z values:

```{r}
#| label: counterfactual-multiple-z
#| message: false
#| warning: false

# Create predictions for different Z values
z_values <- quantile(dag_data$Z, probs = c(0.1, 0.25, 0.5, 0.75, 0.9))
predictions_list <- list()

for (z_val in z_values) {
  y_pred <- predict_counterfactual(x_range, fixed_z = z_val)
  predictions_list[[as.character(round(z_val, 1))]] <- y_pred
}

# Convert to a data frame for ggplot
predictions_df <- data.frame(X = x_range)
for (z_val in names(predictions_list)) {
  predictions_df[[paste0("Z_", z_val)]] <- predictions_list[[z_val]]
}

# Reshape for plotting
predictions_long <- predictions_df %>%
  pivot_longer(cols = starts_with("Z_"), 
               names_to = "Z_value", 
               values_to = "Y_pred") %>%
  mutate(Z_value = gsub("Z_", "", Z_value))

# Plot the counterfactual predictions for different Z values
ggplot() +
  # Add actual data points
  geom_point(data = dag_data, aes(x = X, y = Y), alpha = 0.1, color = "gray") +
  # Add counterfactual predictions
  geom_line(data = predictions_long, 
            aes(x = X, y = Y_pred, color = Z_value, group = Z_value),
            size = 1.0) +
  scale_color_viridis_d(name = "Z Value\n(Age)") +
  labs(
    title = "Counterfactual Predictions for Different Z Values",
    subtitle = "Each line shows the causal effect of X on Y for a specific Z value",
    x = "X (Exposure, e.g., Coffee Consumption)",
    y = "Y (Outcome, e.g., Heart Disease Risk)"
  ) +
  theme_minimal()
```

This visualization demonstrates an important feature of causal inference: we can predict the effect of interventions under different scenarios or for different subgroups. Each line shows how Y would respond to changes in X for a specific value of Z. This type of analysis can be valuable for personalized predictions or targeted interventions.

## 12. Practical Implications and Conclusions

Our analysis of the fork structure DAG with synthetic data demonstrates several key principles of causal inference:

### 12.1 Summary of Key Findings

1. **Importance of adjusting for confounders**: We showed that failing to control for confounder Z leads to a biased estimate of the effect of X on Y. Our statistical tests confirmed that the unadjusted estimate is significantly different from the true causal effect, while the adjusted estimate successfully recovers it.

2. **Backdoor criterion in action**: By conditioning on Z, we blocked the backdoor path X ← Z → Y, allowing us to estimate the true causal effect of X on Y. This was visualized through our residuals analysis, which demonstrated how removing Z's influence isolates the direct X → Y relationship.

3. **D-separation properties**: We verified that X and Y remain associated even after conditioning on Z, confirming the direct causal relationship between X and Y.

4. **Quantifying unmeasured confounding**: Our sensitivity analysis demonstrated how unmeasured confounding can bias causal estimates, emphasizing the importance of identifying and measuring key confounders. The bias increased progressively with the strength of confounding.

5. **Counterfactual prediction**: We used our causal model to predict what would happen to Y if we intervened to change X while holding Z constant—a key capability of causal inference that goes beyond standard predictive modeling.

### 12.2 Real-World Application Context

In a real-world setting like our coffee consumption and heart disease example:

- **X (coffee consumption)** might be measured in cups per day
- **Y (heart disease risk)** might be measured as a risk score or incidence rate
- **Z (age)** would be measured in years

Our analysis helps us understand whether coffee consumption truly affects heart disease risk, or whether the observed association is partially or entirely due to the confounding effect of age.

For example, without adjusting for age, we might conclude that drinking more coffee increases heart disease risk, when in reality older people simply tend to both drink more coffee and have higher heart disease risk. The adjusted analysis reveals the true causal relationship, which could potentially show that coffee has a smaller effect or even a protective effect once age is properly accounted for.

### 12.3 Methodological Insights

Our analysis demonstrated several powerful techniques for causal inference:

1. **Multiple modeling approaches**: We used both frequentist (linear regression with adjustment) and Bayesian (quap) methods, showing how different statistical frameworks can be applied to causal questions.

2. **Visualization of confounding**: Our residual plots provided an intuitive way to understand how controlling for a confounder changes the estimated relationship between X and Y.

3. **Sensitivity analysis**: We quantified how unmeasured confounding could affect our conclusions, an important step for assessing the robustness of causal claims.

4. **Counterfactual prediction**: We showed how to generate predictions for hypothetical interventions, a unique capability of causal models compared to purely predictive models.

### 12.4 Limitations and Extensions

This simple fork structure is just one of many possible causal patterns. Real-world causal relationships often involve:

- Multiple confounders
- Mediator variables (on the causal path between X and Y)
- Collider variables (affected by both X and Y)
- Instrumental variables (affecting X but not directly affecting Y)
- Cyclic relationships (feedback loops)

More complex DAGs would require more sophisticated analysis techniques, but the principles demonstrated here form the foundation of causal inference with DAGs.

### 12.5 Recommendations for Empirical Research

Based on our analysis, we recommend the following practices for causal inference in empirical research:

1. **Draw out your causal model**: Explicitly representing causal assumptions in a DAG forces clarity of thinking and helps identify potential sources of bias.

2. **Identify minimal sufficient adjustment sets**: Use tools like `dagitty` to determine which variables need to be controlled for to estimate causal effects.

3. **Test implied conditional independencies**: Use your DAG to derive testable predictions about conditional independence relationships, and check these against your data.

4. **Conduct sensitivity analyses**: Assess how robust your causal conclusions are to potential unmeasured confounding or model misspecification.

5. **Think counterfactually**: Frame causal questions in terms of "what would happen if we intervened" rather than just associations.

By following these principles and using the methods demonstrated in this analysis, researchers can move beyond correlation to make stronger causal claims from observational data.




## 13. Simpson's Paradox Detection Analysis

Simpson's Paradox occurs when the direction of an association between two variables reverses when conditioning on a third variable. In our fork structure DAG, we can examine whether this phenomenon occurs by stratifying our data based on different levels of the confounder Z.

Think of Simpson's Paradox like a mirage in causal inference - what appears to be true at the surface level completely reverses when you look deeper. In our coffee and heart disease example, we might find that overall, coffee consumption appears harmful, but within each age group, coffee is actually protective.

### 13.1 Simpson's Paradox Detection Analysis

First, let's examine the overall relationship between X and Y, and then compare it to the relationship within different strata of Z:

```{r}
#| label: detect-simpsons-paradox
#| message: false
#| warning: false

# Calculate overall correlation between X and Y
overall_cor <- cor(dag_data$X, dag_data$Y)
overall_slope <- coef(lm(Y ~ X, data = dag_data))[2]

# Create Z quartiles for stratified analysis
dag_data$Z_quartile <- cut(dag_data$Z, 
                          breaks = quantile(dag_data$Z, probs = c(0, 0.25, 0.5, 0.75, 1)), 
                          labels = c("Q1 (Low)", "Q2", "Q3", "Q4 (High)"),
                          include.lowest = TRUE)

# Calculate correlations and slopes within each Z quartile
stratified_results <- dag_data %>%
  group_by(Z_quartile) %>%
  summarise(
    n = n(),
    cor_XY = cor(X, Y),
    slope_XY = coef(lm(Y ~ X))[2],
    mean_Z = mean(Z),
    mean_X = mean(X),
    mean_Y = mean(Y),
    .groups = 'drop'
  )

# Add overall results for comparison
overall_results <- data.frame(
  Z_quartile = "Overall",
  n = nrow(dag_data),
  cor_XY = overall_cor,
  slope_XY = overall_slope,
  mean_Z = mean(dag_data$Z),
  mean_X = mean(dag_data$X),
  mean_Y = mean(dag_data$Y)
)

# Combine results
simpson_analysis <- bind_rows(overall_results, stratified_results)

# Round for display
simpson_analysis <- simpson_analysis %>%
  mutate(across(where(is.numeric), ~ round(., 3)))

# Display results
DT::datatable(simpson_analysis,
              caption = "Simpson's Paradox Detection: Overall vs Stratified Analysis",
              options = list(pageLength = 10, scrollX = TRUE),
              colnames = c("Z Group", "N", "Correlation X-Y", "Slope X→Y", 
                          "Mean Z", "Mean X", "Mean Y")) %>%
  DT::formatRound(columns = c("cor_XY", "slope_XY", "mean_Z", "mean_X", "mean_Y"), digits = 3)
```

### 13.2 Testing for Simpson's Paradox by Status

Let's formally test whether Simpson's Paradox is present by examining if the direction of association changes:

```{r}
#| label: test-simpsons-paradox-status
#| message: false
#| warning: false

# Function to determine Simpson's Paradox status
detect_simpson_status <- function(overall_slope, stratified_slopes) {
  # Check if all stratified slopes have the same sign
  stratified_signs <- sign(stratified_slopes)
  overall_sign <- sign(overall_slope)
  
  # Simpson's Paradox occurs when overall sign differs from all stratified signs
  if (all(stratified_signs == stratified_signs[1]) && 
      overall_sign != stratified_signs[1]) {
    return("Strong Simpson's Paradox")
  } else if (any(stratified_signs != overall_sign)) {
    return("Partial Simpson's Paradox")
  } else {
    return("No Simpson's Paradox")
  }
}

# Get stratified slopes (excluding overall)
stratified_slopes <- stratified_results$slope_XY

# Determine Simpson's Paradox status
simpson_status <- detect_simpson_status(overall_slope, stratified_slopes)

# Create summary table
simpson_summary <- data.frame(
  Measure = c("Overall Slope", "Mean Stratified Slope", "Range of Stratified Slopes",
              "Overall Correlation", "Mean Stratified Correlation", "Simpson's Paradox Status"),
  Value = c(
    round(overall_slope, 3),
    round(mean(stratified_slopes), 3),
    paste(round(min(stratified_slopes), 3), "to", round(max(stratified_slopes), 3)),
    round(overall_cor, 3),
    round(mean(stratified_results$cor_XY), 3),
    simpson_status
  )
)

# Display summary
DT::datatable(simpson_summary,
              caption = "Simpson's Paradox Status Summary",
              options = list(pageLength = 10, dom = 't'),
              colnames = c("Measure", "Value"))
```

### 13.3 Testing for Simpson's Paradox by Groups

Let's examine the phenomenon more systematically by creating binary high/low groups for Z:

```{r}
#| label: test-simpsons-by-groups
#| message: false
#| warning: false

# Create binary Z groups (high/low based on median split)
dag_data$Z_binary <- ifelse(dag_data$Z > median(dag_data$Z), "High Z", "Low Z")

# Calculate slopes and correlations for binary groups
binary_analysis <- dag_data %>%
  group_by(Z_binary) %>%
  summarise(
    n = n(),
    cor_XY = cor(X, Y),
    slope_XY = coef(lm(Y ~ X))[2],
    intercept_XY = coef(lm(Y ~ X))[1],
    mean_Z = mean(Z),
    mean_X = mean(X),
    mean_Y = mean(Y),
    .groups = 'drop'
  )

# Add confidence intervals for slopes
binary_analysis$slope_se <- NA
binary_analysis$slope_ci_lower <- NA
binary_analysis$slope_ci_upper <- NA

for (group in unique(dag_data$Z_binary)) {
  group_data <- dag_data[dag_data$Z_binary == group, ]
  model <- lm(Y ~ X, data = group_data)
  slope_se <- summary(model)$coefficients["X", "Std. Error"]
  slope_ci <- confint(model)["X", ]
  
  binary_analysis[binary_analysis$Z_binary == group, "slope_se"] <- slope_se
  binary_analysis[binary_analysis$Z_binary == group, "slope_ci_lower"] <- slope_ci[1]
  binary_analysis[binary_analysis$Z_binary == group, "slope_ci_upper"] <- slope_ci[2]
}

# Round for display
binary_analysis <- binary_analysis %>%
  mutate(across(where(is.numeric), ~ round(., 3)))

# Display binary group analysis
DT::datatable(binary_analysis,
              caption = "Binary Group Analysis for Simpson's Paradox Detection",
              options = list(pageLength = 10, scrollX = TRUE),
              colnames = c("Z Group", "N", "Correlation", "Slope", "Intercept", 
                          "Mean Z", "Mean X", "Mean Y", "Slope SE", "CI Lower", "CI Upper")) %>%
  DT::formatRound(columns = c("cor_XY", "slope_XY", "intercept_XY", "mean_Z", 
                              "mean_X", "mean_Y", "slope_se", "slope_ci_lower", "slope_ci_upper"), 
                  digits = 3)
```

### 13.4 Visual Detection of Simpson's Paradox

Let's create visualizations to clearly demonstrate whether Simpson's Paradox is present in our data:

```{r}
#| label: visualize-simpsons-paradox
#| fig-cap: "Visual Detection of Simpson's Paradox"
#| fig-subcap: 
#|   - "Overall relationship vs stratified relationships (quartiles)"
#|   - "Binary group analysis with separate regression lines"
#|   - "Slope comparison across different Z stratifications"
#| layout-ncol: 1

# Plot 1: Overall vs Stratified (Quartiles)
p1 <- ggplot(dag_data, aes(x = X, y = Y)) +
  # Overall regression line
  geom_smooth(method = "lm", formula = y ~ x, color = "red", size = 1.5, 
              linetype = "dashed", se = FALSE) +
  # Stratified regression lines
  geom_smooth(aes(color = Z_quartile), method = "lm", formula = y ~ x, 
              size = 1, se = FALSE) +
  # Points colored by quartile
  geom_point(aes(color = Z_quartile), alpha = 0.6) +
  scale_color_viridis_d(name = "Z Quartile") +
  labs(
    title = "Simpson's Paradox Analysis: Overall vs Stratified Relationships",
    subtitle = paste("Overall slope (red dashed):", round(overall_slope, 3),
                    "| Simpson's Status:", simpson_status),
    x = "X (Exposure)",
    y = "Y (Outcome)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Plot 2: Binary Group Analysis
p2 <- ggplot(dag_data, aes(x = X, y = Y)) +
  # Overall regression line
  geom_smooth(method = "lm", formula = y ~ x, color = "red", size = 1.5, 
              linetype = "dashed", se = FALSE) +
  # Stratified regression lines for binary groups
  geom_smooth(aes(color = Z_binary), method = "lm", formula = y ~ x, 
              size = 1.2, se = TRUE, alpha = 0.3) +
  # Points colored by binary group
  geom_point(aes(color = Z_binary), alpha = 0.6) +
  scale_color_manual(values = c("High Z" = "darkblue", "Low Z" = "darkgreen"),
                     name = "Z Group") +
  labs(
    title = "Binary Group Analysis for Simpson's Paradox",
    subtitle = "Red dashed line: Overall relationship | Colored lines: Group-specific relationships",
    x = "X (Exposure)",
    y = "Y (Outcome)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Plot 3: Slope Comparison
slope_comparison <- data.frame(
  Group = c("Overall", stratified_results$Z_quartile, binary_analysis$Z_binary),
  Slope = c(overall_slope, stratified_results$slope_XY, binary_analysis$slope_XY),
  Type = c("Overall", rep("Quartile", 4), rep("Binary", 2))
)

p3 <- ggplot(slope_comparison, aes(x = reorder(Group, Slope), y = Slope, fill = Type)) +
  geom_col(alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  geom_hline(yintercept = 0.3, linetype = "dotted", color = "red", alpha = 0.7) +
  scale_fill_manual(values = c("Overall" = "red", "Quartile" = "steelblue", "Binary" = "darkgreen")) +
  labs(
    title = "Slope Comparison Across Different Stratifications",
    subtitle = "Red dotted line shows true causal effect (0.3)",
    x = "Group",
    y = "Slope of X → Y"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display plots
p1
p2
p3
```

### 13.5 Formal Statistical Test for Simpson's Paradox

Let's conduct formal statistical tests to quantify the evidence for Simpson's Paradox:

```{r}
#| label: formal-test-simpsons-paradox
#| message: false
#| warning: false

# Function to test for significant differences in slopes
test_slope_differences <- function(overall_slope, overall_se, stratified_slopes, stratified_ses) {
  # Test if overall slope differs significantly from each stratified slope
  z_stats <- (overall_slope - stratified_slopes) / sqrt(overall_se^2 + stratified_ses^2)
  p_values <- 2 * (1 - pnorm(abs(z_stats)))
  
  return(list(z_stats = z_stats, p_values = p_values))
}

# Get standard errors for slopes
overall_model <- lm(Y ~ X, data = dag_data)
overall_se <- summary(overall_model)$coefficients["X", "Std. Error"]

# Get standard errors for stratified models
stratified_ses <- numeric(length(stratified_slopes))
for (i in 1:length(unique(dag_data$Z_quartile))) {
  quartile <- levels(dag_data$Z_quartile)[i]
  quartile_data <- dag_data[dag_data$Z_quartile == quartile, ]
  model <- lm(Y ~ X, data = quartile_data)
  stratified_ses[i] <- summary(model)$coefficients["X", "Std. Error"]
}

# Perform tests
slope_tests <- test_slope_differences(overall_slope, overall_se, stratified_slopes, stratified_ses)

# Create test results table
test_results <- data.frame(
  Comparison = paste("Overall vs", stratified_results$Z_quartile),
  Overall_Slope = rep(round(overall_slope, 3), length(stratified_slopes)),
  Stratified_Slope = round(stratified_slopes, 3),
  Difference = round(overall_slope - stratified_slopes, 3),
  Z_Statistic = round(slope_tests$z_stats, 3),
  P_Value = round(slope_tests$p_values, 3),
  Significant = ifelse(slope_tests$p_values < 0.05, "Yes", "No")
)

# Display test results
DT::datatable(test_results,
              caption = "Formal Statistical Tests for Slope Differences",
              options = list(pageLength = 10, scrollX = TRUE)) %>%
  DT::formatRound(columns = c("Overall_Slope", "Stratified_Slope", "Difference", 
                              "Z_Statistic", "P_Value"), digits = 3)
```

### 13.6 Magnitude of Simpson's Paradox Effect

Let's quantify the magnitude of the Simpson's Paradox effect if it exists:

```{r}
#| label: magnitude-simpsons-effect
#| message: false
#| warning: false

# Calculate Simpson's Paradox magnitude measures
simpson_magnitude <- data.frame(
  Measure = c(
    "Overall Slope",
    "Average Stratified Slope",
    "Absolute Difference",
    "Relative Difference (%)",
    "Direction Reversal",
    "Weighted Average Slope",
    "Simpson's Paradox Strength"
  ),
  Value = c(
    round(overall_slope, 3),
    round(mean(stratified_slopes), 3),
    round(abs(overall_slope - mean(stratified_slopes)), 3),
    round(100 * abs(overall_slope - mean(stratified_slopes)) / abs(mean(stratified_slopes)), 3),
    ifelse(sign(overall_slope) != sign(mean(stratified_slopes)), "Yes", "No"),
    # Weighted average by group size
    round(sum(stratified_results$slope_XY * stratified_results$n) / sum(stratified_results$n), 3),
    # Simpson's strength: how much the overall deviates from weighted average
    round(abs(overall_slope - sum(stratified_results$slope_XY * stratified_results$n) / sum(stratified_results$n)), 3)
  )
)

# Display magnitude analysis
DT::datatable(simpson_magnitude,
              caption = "Magnitude of Simpson's Paradox Effect",
              options = list(pageLength = 10, dom = 't'),
              colnames = c("Measure", "Value"))
```

### 13.7 Weighted vs Unweighted Analysis

Let's compare weighted and unweighted analyses to understand the role of group sizes:

```{r}
#| label: weighted-unweighted-analysis
#| message: false
#| warning: false

# Calculate weighted statistics
weighted_slope <- sum(stratified_results$slope_XY * stratified_results$n) / sum(stratified_results$n)
weighted_cor <- sum(stratified_results$cor_XY * stratified_results$n) / sum(stratified_results$n)

# Calculate unweighted statistics
unweighted_slope <- mean(stratified_results$slope_XY)
unweighted_cor <- mean(stratified_results$cor_XY)

# Group size analysis
group_size_analysis <- stratified_results %>%
  mutate(
    prop_of_total = round(n / sum(n), 3),
    slope_weight = round(slope_XY * prop_of_total, 3),
    cor_weight = round(cor_XY * prop_of_total, 3)
  ) %>%
  select(Z_quartile, n, prop_of_total, slope_XY, slope_weight, cor_XY, cor_weight)

# Add summary row
summary_row <- data.frame(
  Z_quartile = "Weighted Total",
  n = sum(group_size_analysis$n),
  prop_of_total = 1.000,
  slope_XY = round(weighted_slope, 3),
  slope_weight = round(sum(group_size_analysis$slope_weight), 3),
  cor_XY = round(weighted_cor, 3),
  cor_weight = round(sum(group_size_analysis$cor_weight), 3)
)

group_analysis_complete <- bind_rows(group_size_analysis, summary_row)

# Display weighted analysis
DT::datatable(group_analysis_complete,
              caption = "Weighted vs Unweighted Analysis by Group Size",
              options = list(pageLength = 10, scrollX = TRUE),
              colnames = c("Z Group", "N", "Proportion", "Slope", "Weighted Slope", 
                          "Correlation", "Weighted Correlation")) %>%
  DT::formatRound(columns = c("prop_of_total", "slope_XY", "slope_weight", 
                              "cor_XY", "cor_weight"), digits = 3)

# Summary comparison table
weighting_comparison <- data.frame(
  Analysis_Type = c("Overall (Marginal)", "Unweighted Average", "Weighted Average", 
                   "True Causal Effect"),
  Slope = c(round(overall_slope, 3), round(unweighted_slope, 3), 
           round(weighted_slope, 3), 0.300),
  Correlation = c(round(overall_cor, 3), round(unweighted_cor, 3), 
                 round(weighted_cor, 3), NA)
)

DT::datatable(weighting_comparison,
              caption = "Comparison of Different Analysis Approaches",
              options = list(pageLength = 10, dom = 't'),
              colnames = c("Analysis Type", "Slope Estimate", "Correlation"))
```

### 13.8 Conclusions from Simpson's Paradox Analysis

Based on our comprehensive analysis, summarized findings about Simpson's Paradox in this synthetic dataset:

```{r}
#| label: simpsons-conclusions
#| message: false
#| warning: false

# Calculate key metrics for conclusions
direction_reversal <- sign(overall_slope) != sign(mean(stratified_slopes))
magnitude_difference <- abs(overall_slope - mean(stratified_slopes))
relative_magnitude <- 100 * magnitude_difference / abs(mean(stratified_slopes))

# Create conclusions summary
conclusions_data <- data.frame(
  Finding = c(
    "Simpson's Paradox Present?",
    "Direction of Overall Effect",
    "Direction of Stratified Effects",
    "Magnitude of Difference",
    "Relative Magnitude (%)",
    "Explanation",
    "Causal Interpretation",
    "Practical Implication"
  ),
  Result = c(
    simpson_status,
    ifelse(overall_slope > 0, "Positive", "Negative"),
    ifelse(mean(stratified_slopes) > 0, "Positive", "Negative"),
    paste(round(magnitude_difference, 3), "units"),
    paste(round(relative_magnitude, 1), "%"),
    "Confounding by Z creates different marginal vs conditional relationships",
    "Adjusted analysis reveals true causal effect",
    "Controlling for confounders is essential for causal inference"
  )
)

# Display conclusions
DT::datatable(conclusions_data,
              caption = "Summary of Simpson's Paradox Analysis Conclusions",
              options = list(pageLength = 10, dom = 't', scrollX = TRUE),
              colnames = c("Key Finding", "Result/Interpretation"))
```

Our Simpson's Paradox analysis reveals important insights about the nature of confounding in causal inference. In this synthetic dataset, we observe `r simpson_status`, which demonstrates how the confounder Z creates different patterns when we examine the marginal (overall) versus conditional (stratified) relationships between X and Y.

The key insight is that Simpson's Paradox is not just a statistical curiosity—it's a fundamental illustration of why proper causal analysis requires careful consideration of confounding variables. In our fork structure DAG, the confounder Z affects both the exposure X and outcome Y, creating spurious associations that can mislead us about the true causal relationship.

Think of it this way: if we only looked at the overall relationship between coffee consumption (X) and heart disease (Y), we might reach one conclusion. But when we properly account for age (Z) by examining the relationship within age groups, we see the true causal story. This is why randomized controlled trials—which balance confounders across treatment groups—are so valuable for causal inference.

The magnitude of the Simpson's Paradox effect in our data (`r paste(round(relative_magnitude, 1), "%")` relative difference) underscores the practical importance of identifying and controlling for key confounders in observational studies. This analysis reinforces our earlier findings about the critical role of proper adjustment in causal inference.


## Session Information for Reproducibility

```{r}
#| label: session-info
#| echo: false

# Session information for reproducibility
sessionInfo()

```


  
