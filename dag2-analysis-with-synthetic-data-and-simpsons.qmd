---
title: "DAG Analysis with Synthetic Data - Fork + Collider Structure"
author: "Dan Swart and Claude 4.0"
format: 
  html:
    toc: true
    toc-float: true
    page-layout: article
    embed-resources: true
    code-fold: true
    code-summary: "Show the code"
    code-tools: true
    code-overflow: wrap
    code-block-bg: "#FAEBD7"
    code-block-border-left: "#31BAE9"
    code-link: true          # This adds individual buttons
    fig-width: 10
    fig-height: 8
    fig-align: center
    html-math-method: katex
    css: swart-20250327.css
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    papersize: letter
    geometry:
      - margin=1in
    fig-width: 10
    fig-height: 8
    fig-pos: 'H'
  typst:
    toc: true
    fig-width: 10
    fig-height: 8
    keep-tex: true
    prefer-html: true
---

```{r}
#| include: false

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
                 fig.width = 9, fig.height = 6, out.width = "100%", fig.align = "center")

# Load necessary libraries
library(tidyverse)  # For dplyr, ggplot, and friends
library(ggdag)      # For plotting DAGs
library(dagitty)    # For working with DAG logic
library(DiagrammeR) # For complete control of the layout
library(knitr)      # For controlling rendering
library(kableExtra) # For tables summarizing results
library(DT)         # For rendering interactive tables
library(broom)      # For tidying model results
library(purrr)      # For functional programming
library(lavaan)     # For SEM to test the DAG structure
library(corrplot)   # For correlation matrices
library(viridis)    # For nice color scales
library(rethinking) # For Bayesian methods
```

## Introduction

This document demonstrates how to generate synthetic data based on a DAG with a fork and collider structure, and how to analyze it to verify the causal relationships. The DAG represents the following relationships:

- Z is a confounder (fork) affecting both X and Y (Z → X, Z → Y)
- X directly affects Y (X → Y)
- Both X and Y affect C, creating a collider (X → C ← Y)

Let's start by visualizing the DAG structure we'll be testing.

## 1. DAG Structure

```{r}
#| label: diagrammer-visualization-fork-collider
#| fig-cap: "Directed Acyclic Graph of Fork and Collider Structure"

library(DiagrammeR)

# Create the DAG using DiagrammeR for detailed control
fork_collider_dag_viz <- grViz("
  digraph DAG {
    # Graph settings
    graph [layout=neato, margin=\"0.0, 0.0, 0.0, 0.0\"]
    
    # Add a title
    labelloc=\"t\"
    label=\"Fork and Collider Causal Structure\\nwith Actual Effect Sizes from Synthetic Data\\n   \"
    fontname=\"Cabin\"  fontsize=18
    
   # Node settings
    node [shape=plaintext, fontsize=20, fontname=\"Cabin\"]
    
    # Edge settings
    edge [penwidth=1.20, color=\"darkblue\", arrowsize=1.00, fontsize=12]
    
    # Nodes with coordinates matching the ggdag layout
    X [label=\"X\\n(Exposure)\", pos=\"1.0, 3.0!\", fontcolor=\"dodgerblue\"]
    Y [label=\"Y\\n(Outcome)\", pos=\"5.0, 3.0!\", fontcolor=\"dodgerblue\"]
    Z [label=\"Z\\n(Confounder)\", pos=\"3.0, 5.0!\", fontcolor=\"red\"]
    C [label=\"C\\n(Collider)\", pos=\"3.0, 1.0!\", fontcolor=\"red\"]
    
    # Edges with coefficients from synthetic data generation
    X -> Y [label=\"0.4\"]
    Z -> X [label=\"0.5\"]
    Z -> Y [label=\"0.3\"]
    X -> C [label=\"0.6\"]
    Y -> C [label=\"0.7\"]
    
    # Caption
    Caption [shape=plaintext, label=\"Figure: Fork (Z) and Collider (C) Causal Structure\", 
             fontsize=12, pos=\"2.0, 0.0!\"]
  }
")

# Show the DiagrammeR DAG
fork_collider_dag_viz

```


```{r dag-structure, out.width="100%"}
#| fig-cap: "The conceptual DAG structure with fork (Z) and collider (C)"

library(tidyverse)
library(ggdag)
library(dagitty)

# Create the DAG using ggdag
dag <- dagify(
  Y ~ X + Z,
  X ~ Z,
  C ~ X + Y,
  exposure = "X",
  outcome = "Y"
)

# Set coordinates for visualization
coordinates(dag) <- list(
  x = c(X = 1, Y = 3, Z = 2, C = 2),
  y = c(X = 2, Y = 2, Z = 3, C = 1)
)

# Visualize the DAG
ggdag(dag) + 
  theme_dag() +
  ggtitle("DAG: X -> Y with Z as confounder and C as collider") +
  theme(plot.title = element_text(size = 14, hjust = 0.5),
        plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm"))

```

## 2. Generating Synthetic Data

We'll generate synthetic data that follows the causal structure defined in our DAG. The data generation process will ensure that the relationships match the theoretical paths.

```{r generate-data, out.width="100%"}
#| fig-cap: "Correlation plot of synthetic data"

library(tidyverse)
library(ggdag)
library(dagitty)
library(DT)


set.seed(2025) # For reproducibility

# Number of observations
n <- 1000

# Generate data according to the DAG structure
# 1. Start with the exogenous variable Z

Z <- rnorm(n, mean = 50, sd = 10)
Z <- round(Z, 3)  # Round to 4 decimal places

# 2. X depends on Z (fork)
X <- 0.5 * Z + rnorm(n, mean = 0, sd = 5)
X <- round(X, 3)  # Round to 4 decimal places

# 3. Y depends on both X and Z (fork and direct effect)
Y <- 0.4 * X + 0.3 * Z + rnorm(n, mean = 0, sd = 5)
Y <- round(Y, 3)  # Round to 4 decimal places

# 4. C depends on both X and Y (collider)
C <- 0.6 * X + 0.7 * Y + rnorm(n, mean = 0, sd = 5)
C <- round(C, 3)  # Round to 4 decimal places

# Combine into a data frame
dag_data <- tibble(X = X, Y = Y, Z = Z, C = C)


DT::datatable(head(dag_data),
              options = list(
                pageLength = 6,
                searching = TRUE,
                info = FALSE,
                paging = FALSE,
                scrollX = TRUE  # Enable horizontal scrolling if needed
                             ),
              class = 'cell-border stripe compact responsive',
              rownames = FALSE,
              width = "90%"
              )

# Correlation matrix visualization to verify relationships
cor_matrix <- cor(dag_data)

# Use the built-in color sequences in corrplot
corrplot::corrplot(cor_matrix, 
                  method = "color", 
                  type = "upper",
                  addCoef.col = "black", 
                  number.cex = 1.5,
                  tl.cex = 1.2,
                  tl.col = "darkblue", 
                  tl.srt = 45,
                  col = colorRampPalette(c("white", "lightgreen", "pink", "orange", "lightblue"))(5),
                  diag = TRUE,
                  title = "Correlation Matrix of Synthetic Data",
                  mar = c(0, 0, 2, 0))  # Add this line: increases top margin for title

# Add a description of the correlation levels
text(x = 0.5, y = -0.1, 
     labels = "Color legend: white (<0.2) → lightgreen (0.2-0.4) → pink (0.4-0.6) → orange (0.6-0.8) → lightblue (>0.8)",
     cex = 1.0)

```

## 3. Examining Marginal Relationships

Let's first look at the marginal relationships between variables without any conditioning.

```{r marginal-relationships, out.width="100%"}
#| fig-cap: "Scatter plots showing marginal relationships between variables"

# Create a panel of scatter plots
dag_data %>%
  pivot_longer(-X, names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = X, y = value)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  facet_wrap(~ variable, scales = "free") +
  labs(title = "Relationship between X and other variables",
       y = "Variable Value")

# Also look at Z's relationship with Y
ggplot(dag_data, aes(x = Z, y = Y)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "Relationship between Z and Y (Confounder Effect)")
```

## 4. Testing the Fork Structure (Confounder)

Let's examine how Z acts as a confounder for the X-Y relationship by comparing models with and without adjusting for Z.

```{r test-fork}
#| tbl-cap: "Comparing models with and without adjusting for the confounder Z"

# Model without adjusting for Z (naive model)
model_naive <- lm(Y ~ X, data = dag_data)

# Model properly adjusting for Z (fork)
model_adjusted <- lm(Y ~ X + Z, data = dag_data)

# Compare coefficients
models_comparison <- bind_rows(
  tidy(model_naive) %>% mutate(model = "Naive (Y ~ X)"),
  tidy(model_adjusted) %>% mutate(model = "Adjusted (Y ~ X + Z)")
) %>%
  filter(term == "X") %>%
  select(model, term, estimate, std.error, p.value) %>% 
  mutate(across(where(is.numeric), ~round(., 3)))  # Round all numeric columns to 4 decimal places

# Display comparison
DT::datatable(models_comparison,
              options = list(
                pageLength = 10,
                ordering = TRUE,
                searching = FALSE,
                scrollX = TRUE
              ),
              class = 'cell-border stripe compact responsive',
              rownames = FALSE,
              width = "90%")
```

### A2. Residual Diagnostics

Let's check the residuals of our correctly specified model to ensure the model assumptions are met.

```{r residual-diagnostics, out.width="100%"}
#| fig-cap: "Residual diagnostics for the correctly specified model"

# Make sure the models object is available in this scope
models <- list(
  "None" = lm(Y ~ X, data = dag_data),
  "Z" = lm(Y ~ X + Z, data = dag_data),
  "C" = lm(Y ~ X + C, data = dag_data),
  "Z, C" = lm(Y ~ X + Z + C, data = dag_data)
)

# Get the correctly specified model
correct_model <- models[["Z"]]

# Plot diagnostics
par(mfrow = c(2, 2))
plot(correct_model)
```

The residual diagnostics confirm that our model assumptions are reasonably met, supporting the validity of our causal effect estimates.

```{r}
# Calculate the confounding bias
bias <- models_comparison$estimate[1] - models_comparison$estimate[2]
bias_percent <- (bias / models_comparison$estimate[2]) * 100

cat("Confounding bias in the X coefficient:", round(bias, 3), "\n")
cat("Percent bias:", round(bias_percent, 1), "%\n")
```

The comparison above demonstrates the confounding effect of Z. The naive model without adjusting for Z overestimates the causal effect of X on Y because it captures both the direct effect and the indirect effect through the confounder.

## 5. Testing the Collider Structure

Now, let's examine how conditioning on C (a collider) affects the relationship between X and Y.

```{r test-collider}
#| tbl-cap: "Effect of conditioning on the collider C"

# First, check if X and Y have the expected relationship in data
model_true <- lm(Y ~ X + Z, data = dag_data)

# Now, erroneously condition on the collider C
model_collider_bias <- lm(Y ~ X + Z + C, data = dag_data)

# Compare coefficient on X
collider_comparison <- bind_rows(
  tidy(model_true) %>% mutate(model = "Correct (Y ~ X + Z)"),
  tidy(model_collider_bias) %>% mutate(model = "Collider Bias (Y ~ X + Z + C)")
) %>%
  filter(term == "X") %>%
  select(model, term, estimate, std.error, p.value) %>% 
  mutate(across(where(is.numeric), ~round(., 3)))  # Round all numeric columns to 4 decimal places

# Display comparison
DT::datatable(collider_comparison,
              options = list(
                pageLength = 10,
                ordering = TRUE,
                searching = FALSE,
                scrollX = TRUE
              ),
              class = 'cell-border stripe compact responsive',
              rownames = FALSE,
              width = "90%")

# Calculate the collider bias
collider_bias <- collider_comparison$estimate[2] - collider_comparison$estimate[1]
collider_bias_percent <- (collider_bias / collider_comparison$estimate[1]) * 100

cat("Collider bias in the X coefficient:", round(collider_bias, 3), "\n")
cat("Percent bias:", round(collider_bias_percent, 1), "%\n")
```

The results show how conditioning on the collider C distorts the estimated causal effect of X on Y. This is a classic example of collider bias.

## 6. Stratification Analysis

Let's visualize how the relationship between X and Y changes across different strata of Z (confounder) and C (collider).

```{r stratification, out.width="100%"}
#| fig-cap: "Stratified analysis showing effects of Z and C"
#| fig-subcap: 
#|   - "Relationship between X and Y stratified by Z"
#|   - "Relationship between X and Y stratified by C"
#| layout-ncol: 1

# Create Z strata
dag_data <- dag_data %>%
  mutate(Z_strata = cut(Z, breaks = 3, labels = c("Low Z", "Medium Z", "High Z")))

# Stratified analysis by Z (confounder)
ggplot(dag_data, aes(x = X, y = Y, color = Z_strata)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ Z_strata) +
  labs(title = "X-Y Relationship Stratified by Z (Confounder)",
       subtitle = "Properly adjusting for the confounder") +
  theme(legend.position = "bottom")

# Create C strata
dag_data <- dag_data %>%
  mutate(C_strata = cut(C, breaks = 3, labels = c("Low C", "Medium C", "High C")))

# Stratified analysis by C (collider)
ggplot(dag_data, aes(x = X, y = Y, color = C_strata)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~ C_strata) +
  labs(title = "X-Y Relationship Stratified by C (Collider)",
       subtitle = "Illustrating collider bias when stratifying incorrectly") +
  theme(legend.position = "bottom")
```

The stratification analysis visually demonstrates:

1. When we stratify by Z (confounder), we see a more consistent relationship between X and Y within each stratum, representing the true causal effect.
2. When we stratify by C (collider), we see varying relationships across strata, illustrating how conditioning on a collider can distort the true relationship.

## 7. Path Analysis using SEM

We can use structural equation modeling (SEM) to test the full DAG structure and estimate all path coefficients simultaneously.

```{r sem-analysis}
#| tbl-cap: "SEM Path Analysis Results"

# Define the SEM model based on our DAG
sem_model <- '
  # Direct effects
  X ~ a*Z
  Y ~ b*X + c*Z
  C ~ d*X + e*Y
  
  # Indirect effects
  XY_indirect := a*b
  total := b + XY_indirect
'

# Fit the model
sem_fit <- sem(sem_model, data = dag_data)

# Display the results
summary(sem_fit, standardized = TRUE, fit.measures = TRUE)

# Extract and display path coefficients
sem_coefs <- parameterEstimates(sem_fit) %>%
  filter(op %in% c("~", ":=")) %>%
  select(lhs, op, rhs, est, se, z, pvalue, ci.lower, ci.upper)

# Create a data frame with the formatted columns first
sem_results <- sem_coefs %>%
  mutate(
    Path = case_when(
      op == "~" ~ paste(lhs, "<-", rhs),
      op == ":=" & rhs == "a*b" ~ "Indirect effect (X <- Z -> Y)",
      op == ":=" & rhs == "b + XY_indirect" ~ "Total effect of X on Y",
      TRUE ~ paste(lhs, op, rhs)
    ),
    Estimate = round(est, 3),
    SE = round(se, 3),
    `Z-value` = round(z, 3),
    `P-value` = round(pvalue, 3),
    `95% CI` = paste0("[", round(ci.lower, 3), ", ", round(ci.upper, 3), "]")
  ) %>%
  select(Path, Estimate, SE, `Z-value`, `P-value`, `95% CI`)

# Display the results table
DT::datatable(
  sem_results,
  options = list(
    pageLength = 10,
    ordering = TRUE,
    searching = FALSE,
    scrollX = TRUE
  ),
 class = 'cell-border stripe compact responsive',
 rownames = FALSE,
 width = "90%"
)
```

The SEM analysis confirms the structural relationships in our DAG and provides estimates for all paths, including the indirect effect through the confounder.

## 8. Testing D-Separation Claims

Let's test the conditional independence claims implied by the DAG using partial correlations.

```{r d-separation}
#| tbl-cap: "D-Separation Tests"

# Let's rewrite the d-separation testing code completely to avoid the issues

# Create manual tests for key relationships in our DAG
# 1. Test X and Y without conditioning (should be correlated)
cor_xy <- cor.test(dag_data$X, dag_data$Y)

# 2. Test X and Y conditioning on Z (should still be correlated due to direct effect)
# First fit X ~ Z
model_x_z <- lm(X ~ Z, data = dag_data)
resid_x_given_z <- residuals(model_x_z)

# Then fit Y ~ Z
model_y_z <- lm(Y ~ Z, data = dag_data)
resid_y_given_z <- residuals(model_y_z)

# Test correlation between residuals
cor_xy_given_z <- cor.test(resid_x_given_z, resid_y_given_z)

# 3. Test X and Y conditioning on C (should be correlated due to collider bias)
model_x_c <- lm(X ~ C, data = dag_data)
resid_x_given_c <- residuals(model_x_c)

model_y_c <- lm(Y ~ C, data = dag_data)
resid_y_given_c <- residuals(model_y_c)

cor_xy_given_c <- cor.test(resid_x_given_c, resid_y_given_c)

# 4. Test X and Y conditioning on both Z and C
model_x_zc <- lm(X ~ Z + C, data = dag_data)
resid_x_given_zc <- residuals(model_x_zc)

model_y_zc <- lm(Y ~ Z + C, data = dag_data)
resid_y_given_zc <- residuals(model_y_zc)

cor_xy_given_zc <- cor.test(resid_x_given_zc, resid_y_given_zc)

# Compile results
independence_results <- tibble(
  Claim = c("X ⊥ Y", "X ⊥ Y | Z", "X ⊥ Y | C", "X ⊥ Y | Z,C"),
  Correlation = c(
    cor_xy$estimate, 
    cor_xy_given_z$estimate,
    cor_xy_given_c$estimate,
    cor_xy_given_zc$estimate
  ),
  `P-value` = c(
    cor_xy$p.value,
    cor_xy_given_z$p.value,
    cor_xy_given_c$p.value,
    cor_xy_given_zc$p.value
  ),
  Independent = c(
    cor_xy$p.value > 0.05,
    cor_xy_given_z$p.value > 0.05,
    cor_xy_given_c$p.value > 0.05,
    cor_xy_given_zc$p.value > 0.05
  )
)

# Display the results
DT::datatable(independence_results %>% 
                mutate(across(c(Correlation, `P-value`), ~round(., 3))),
              options = list(
                pageLength = 10,
                ordering = TRUE,
                searching = FALSE,
                scrollX = TRUE
              ),
              class = 'cell-border stripe compact responsive',
              rownames = FALSE,
              width = "90%")
```

The d-separation tests reveal:

1. When we condition on Z (the confounder), the correlation between X and Y reflects only the direct causal effect.
2. When we condition on C (the collider), we may see a distorted relationship between X and Y due to collider bias.
3. When we condition on both Z and C, we properly adjust for confounding but introduce collider bias.

## 9. Evaluating Bias Under Different Adjustment Strategies

Finally, let's systematically evaluate the bias in estimating the causal effect of X on Y under different adjustment strategies.

```{r adjustment-strategies}
#| tbl-cap: "Comparison of Different Adjustment Strategies"

# Create different models representing adjustment strategies
models <- list(
  "None" = lm(Y ~ X, data = dag_data),
  "Z" = lm(Y ~ X + Z, data = dag_data),
  "C" = lm(Y ~ X + C, data = dag_data),
  "Z, C" = lm(Y ~ X + Z + C, data = dag_data)
)

# Extract coefficients for X (without rounding yet)
adjustment_results <- tibble(
  `Adjustment Set` = names(models),
  `X Coefficient` = sapply(models, function(m) coef(m)["X"]),
  `Std. Error` = sapply(models, function(m) summary(m)$coefficients["X", "Std. Error"]),
  `t-value` = sapply(models, function(m) summary(m)$coefficients["X", "t value"]),
  `p-value` = sapply(models, function(m) summary(m)$coefficients["X", "Pr(>|t|)"]),
  `R-squared` = sapply(models, function(m) summary(m)$r.squared)
)

# Calculate bias relative to the correctly specified model (adjusting for Z only)
true_effect <- adjustment_results$`X Coefficient`[adjustment_results$`Adjustment Set` == "Z"]

adjustment_results <- adjustment_results %>%
  mutate(
    Bias = `X Coefficient` - true_effect,
    `Percent Bias` = (Bias / true_effect) * 100
  ) %>%
  mutate(across(where(is.numeric), ~round(., 3)))  # Round all numeric columns to 4 decimal places

# Display the results
DT::datatable(adjustment_results,
              options = list(
                pageLength = 10,
                ordering = TRUE,
                searching = FALSE,
                scrollX = TRUE
              ),
              class = 'cell-border stripe compact responsive',
              rownames = FALSE,
              width = "90%")
```

## 10. Visualization of Causal Effects Under Different Adjustments

Let's visualize how the estimated causal effect of X on Y changes under different adjustment strategies.

```{r visualize-effects, out.width="100%"}
#| fig-cap: "Visualization of causal effect estimates under different adjustment strategies"

# Create a forest plot of X coefficients
adjustment_results %>%
  mutate(`Adjustment Set` = factor(`Adjustment Set`, 
                                  levels = c("None", "Z", "C", "Z, C"))) %>%
  ggplot(aes(x = `X Coefficient`, y = `Adjustment Set`, 
             xmin = `X Coefficient` - 1.96 * `Std. Error`, 
             xmax = `X Coefficient` + 1.96 * `Std. Error`,
             color = `Adjustment Set` == "Z")) +
  geom_pointrange() +
  geom_vline(xintercept = true_effect, linetype = "dashed", color = "darkgreen") +
  scale_color_manual(values = c("red", "darkgreen")) +
  labs(title = "Causal Effect Estimates Under Different Adjustment Strategies",
       subtitle = "Dashed line represents the true causal effect (adjusting for Z only)",
       x = "Estimated Causal Effect of X on Y",
       y = "Adjustment Strategy") +
  theme_minimal() +
  theme(legend.position = "none")
```

## 11. Bayesian Causal Inference Analysis

In addition to the frequentist approach we've used so far, we can apply Bayesian methods to estimate causal effects in our DAG. Bayesian inference provides several advantages for causal analysis:

1. It expresses uncertainty through complete probability distributions rather than just point estimates
2. It allows incorporation of prior knowledge about causal relationships
3. It provides a more nuanced interpretation of uncertainty in causal effects

Let's implement this approach using the `rethinking` package:

```{r}
#| label: bayesian-causal-analysis
#| message: false
#| warning: false
#| results: 'hide'

# Load required packages if not already loaded
if (!require("rethinking")) {
  # Install if needed (uncomment if needed)
  # install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
  # install.packages("rethinking", dependencies = TRUE, repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
  library(rethinking)
}

# Standardize variables for better model fitting
# First check which variables exist in the dataset
dag_var_names <- names(dag_data)
print(paste("Variables in dag_data:", paste(dag_var_names, collapse=", ")))

# Standardize all numeric variables in the dataset
dag_data_std <- dag_data %>%
  mutate(across(where(is.numeric), scale))

# Define and fit Bayesian models for different adjustment sets
# 1. No adjustment (biased)
m_none <- quap(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + bX * X,
    a ~ dnorm(0, 1),
    bX ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = dag_data_std
)

# 2. Adjusting for Z (confounder)
m_adjusted <- quap(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + bX * X + bZ * Z,
    a ~ dnorm(0, 1),
    bX ~ dnorm(0, 1),
    bZ ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = dag_data_std
)

# 3. Full model with collider (typically not recommended)
m_full <- quap(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + bX * X + bZ * Z + bC * C,
    a ~ dnorm(0, 1),
    bX ~ dnorm(0, 1),
    bZ ~ dnorm(0, 1),
    bC ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = dag_data_std
)
```

Now let's extract the posterior distributions and compile the results:

```{r}
#| label: extract-bayesian-posteriors

# Extract samples from the posterior distributions
post_none <- extract.samples(m_none)
post_adjusted <- extract.samples(m_adjusted)
post_full <- extract.samples(m_full)

# Create a function to summarize posteriors
summarize_posterior <- function(posterior, name) {
  data.frame(
    Adjustment_Set = name,  # Using underscore to avoid conversion issues
    Mean = mean(posterior$bX),
    Median = median(posterior$bX),
    SD = sd(posterior$bX),
    CI_Lower = quantile(posterior$bX, 0.025),
    CI_Upper = quantile(posterior$bX, 0.975),
    Width = quantile(posterior$bX, 0.975) - quantile(posterior$bX, 0.025)
  )
}

# Summarize the models
bayesian_results <- rbind(
  summarize_posterior(post_none, "None"),
  summarize_posterior(post_adjusted, "Z only"),
  summarize_posterior(post_full, "Z and C")
)

# Display the results using DT
library(DT)
datatable(bayesian_results, 
          caption = "Bayesian estimates of the causal effect of X on Y under different adjustment strategies",
          options = list(pageLength = 5, dom = 't'),
          rownames = FALSE) %>%
  formatRound(columns = c("Mean", "Median", "SD", "CI_Lower", "CI_Upper", "Width"), digits = 3)
```

Let's visualize the posterior distributions to better understand the uncertainty in our causal effect estimates:

```{r}
#| label: plot-bayesian-posteriors
#| fig-width: 8
#| fig-height: 6
#| fig-cap: "Posterior distributions of causal effect estimates under different adjustment strategies"

# Create a data frame with the posterior samples
all_posteriors <- data.frame(
  None = post_none$bX,
  Z_only = post_adjusted$bX,
  Z_and_C = post_full$bX
)

# Convert to long format for plotting
library(tidyr)
long_posteriors <- all_posteriors %>%
  pivot_longer(cols = everything(), 
               names_to = "Adjustment_Set", 
               values_to = "Effect_Estimate")

# Set factor levels for consistent ordering
long_posteriors$Adjustment_Set <- factor(long_posteriors$Adjustment_Set,
                                        levels = c("None", "Z_only", "Z_and_C"))

# Plot density curves for all adjustment sets
ggplot(long_posteriors, aes(x = Effect_Estimate, fill = Adjustment_Set)) +
  geom_density(alpha = 0.5) +
  geom_vline(data = bayesian_results, 
             aes(xintercept = Mean, color = Adjustment_Set),
             linetype = "dashed") +
  scale_fill_brewer(palette = "Set1") +
  scale_color_brewer(palette = "Set1") +
  labs(
    title = "Posterior Distributions of the Causal Effect of X on Y",
    subtitle = "Under different adjustment strategies",
    x = "Causal Effect (Standardized)",
    y = "Density"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(title = "Adjustment Strategy"),
         color = guide_legend(title = "Adjustment Strategy"))
```

For a more direct comparison with our frequentist approach, let's create a forest plot:

```{r}
#| label: bayesian-forest-plot
#| fig-width: 8
#| fig-height: 5
#| fig-cap: "Forest plot of Bayesian causal effect estimates under different adjustment strategies"

# Calculate the true standardized effect for reference
true_effect_std <- 0.4 / sd(dag_data$Y) * sd(dag_data$X)

# Create forest plot
ggplot(bayesian_results, 
       aes(x = Mean, y = Adjustment_Set, 
           xmin = CI_Lower, xmax = CI_Upper,
           color = Adjustment_Set == "Z only")) +
  geom_pointrange(size = 1) +
  geom_vline(xintercept = true_effect_std, linetype = "dashed", color = "darkgreen") +
  scale_color_manual(values = c("red", "darkgreen", "red")) +
  labs(
    title = "Bayesian Causal Effect Estimates Under Different Adjustment Strategies",
    subtitle = "Green represents the correctly specified adjustment set (Z only)",
    x = "Estimated Causal Effect of X on Y (Standardized)",
    y = "Adjustment Strategy"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

### Interpretation of Bayesian Causal Analysis

The Bayesian analysis provides a more complete picture of uncertainty in our causal effect estimates through posterior probability distributions. Here are the key findings:

1. **Adjustment Strategy Comparisons**: 
   - The posterior distribution for the unadjusted model ("None") shows bias compared to the model adjusting for Z only, confirming our DAG-based conclusion that Z is a confounder.
   - The model adjusting for both Z and C (the collider) shows a distortion of the causal effect, demonstrating collider bias in the Bayesian framework.
   - The model adjusting only for Z provides the most accurate estimate of the true causal effect.

2. **Uncertainty Quantification**:
   - The width of the posterior distributions reveals how each adjustment strategy affects uncertainty.
   - The credible intervals show the range of plausible values for the causal effect under each adjustment strategy.
   - Note how adjusting for the collider C not only shifts the estimate but also affects the uncertainty.

3. **Posterior Probability Interpretation**:
   - Unlike frequentist confidence intervals, the Bayesian 95% credible intervals have a direct probability interpretation: there is a 95% probability that the true causal effect lies within this range.
   - This probabilistic interpretation allows for more intuitive statements about the uncertainty in causal effects.

4. **Agreement with Frequentist Results**:
   - The Bayesian analysis largely confirms our frequentist findings, providing additional confidence in our causal conclusions.
   - The posterior means are similar to the frequentist point estimates, but the Bayesian framework offers a more comprehensive understanding of uncertainty.

5. **Practical Implications**:
   - For causal inference, our Bayesian analysis confirms that adjusting only for the confounder Z is optimal, avoiding both confounding bias and collider bias.
   - The posterior distributions provide a rich framework for communicating uncertainty in causal effects to stakeholders.

This Bayesian approach offers a rich framework for causal inference that goes beyond point estimates, enabling researchers to express and incorporate uncertainty in a more nuanced way. The posterior distributions provide a complete picture of plausible causal effects given our data and modeling assumptions.





## 12. Simpson's Paradox Detection Analysis

Simpson's Paradox occurs when the direction of an association between two variables reverses when conditioning on a third variable. In our fork structure DAG, we can examine whether this phenomenon occurs by stratifying our data based on different levels of the confounder Z.

Think of Simpson's Paradox like a mirage in causal inference - what appears to be true at the surface level completely reverses when you look deeper. In our DAG example, we might find that overall, X appears to have one effect on Y, but within each level of the confounder Z, the relationship tells a different story.

### 12.1 Simpson's Paradox Detection Analysis

First, let's examine the overall relationship between X and Y, and then compare it to the relationship within different strata of Z:

```{r}
#| label: detect-simpsons-paradox
#| message: false
#| warning: false

# Calculate overall correlation between X and Y
overall_cor <- cor(dag_data$X, dag_data$Y)
overall_slope <- coef(lm(Y ~ X, data = dag_data))[2]

# Create Z quartiles for stratified analysis
dag_data$Z_quartile <- cut(dag_data$Z, 
                          breaks = quantile(dag_data$Z, probs = c(0, 0.25, 0.5, 0.75, 1)), 
                          labels = c("Q1 (Low)", "Q2", "Q3", "Q4 (High)"),
                          include.lowest = TRUE)

# Calculate correlations and slopes within each Z quartile
stratified_results <- dag_data %>%
  group_by(Z_quartile) %>%
  summarise(
    n = n(),
    cor_XY = cor(X, Y),
    slope_XY = coef(lm(Y ~ X))[2],
    mean_Z = mean(Z),
    mean_X = mean(X),
    mean_Y = mean(Y),
    .groups = 'drop'
  )

# Add overall results for comparison
overall_results <- data.frame(
  Z_quartile = "Overall",
  n = nrow(dag_data),
  cor_XY = overall_cor,
  slope_XY = overall_slope,
  mean_Z = mean(dag_data$Z),
  mean_X = mean(dag_data$X),
  mean_Y = mean(dag_data$Y)
)

# Combine results
simpson_analysis <- bind_rows(overall_results, stratified_results)

# Round for display
simpson_analysis <- simpson_analysis %>%
  mutate(across(where(is.numeric), ~ round(., 3)))

# Display results
DT::datatable(simpson_analysis,
              caption = "Simpson's Paradox Detection: Overall vs Stratified Analysis",
              options = list(pageLength = 10, scrollX = TRUE),
              colnames = c("Z Group", "N", "Correlation X-Y", "Slope X→Y", 
                          "Mean Z", "Mean X", "Mean Y")) %>%
  DT::formatRound(columns = c("cor_XY", "slope_XY", "mean_Z", "mean_X", "mean_Y"), digits = 3)
```

### 12.2 Testing for Simpson's Paradox by Status

Let's formally test whether Simpson's Paradox is present by examining if the direction of association changes:

```{r}
#| label: test-simpsons-paradox-status
#| message: false
#| warning: false

# Function to determine Simpson's Paradox status
detect_simpson_status <- function(overall_slope, stratified_slopes) {
  # Check if all stratified slopes have the same sign
  stratified_signs <- sign(stratified_slopes)
  overall_sign <- sign(overall_slope)
  
  # Simpson's Paradox occurs when overall sign differs from all stratified signs
  if (all(stratified_signs == stratified_signs[1]) && 
      overall_sign != stratified_signs[1]) {
    return("Strong Simpson's Paradox")
  } else if (any(stratified_signs != overall_sign)) {
    return("Partial Simpson's Paradox")
  } else {
    return("No Simpson's Paradox")
  }
}

# Get stratified slopes (excluding overall)
stratified_slopes <- stratified_results$slope_XY

# Determine Simpson's Paradox status
simpson_status <- detect_simpson_status(overall_slope, stratified_slopes)

# Create summary table
simpson_summary <- data.frame(
  Measure = c("Overall Slope", "Mean Stratified Slope", "Range of Stratified Slopes",
              "Overall Correlation", "Mean Stratified Correlation", "Simpson's Paradox Status"),
  Value = c(
    round(overall_slope, 3),
    round(mean(stratified_slopes), 3),
    paste(round(min(stratified_slopes), 3), "to", round(max(stratified_slopes), 3)),
    round(overall_cor, 3),
    round(mean(stratified_results$cor_XY), 3),
    simpson_status
  )
)

# Display summary
DT::datatable(simpson_summary,
              caption = "Simpson's Paradox Status Summary",
              options = list(pageLength = 10, dom = 't'),
              colnames = c("Measure", "Value"))
```

### 12.3 Testing for Simpson's Paradox by Groups

Let's examine the phenomenon more systematically by creating binary high/low groups for Z:

```{r}
#| label: test-simpsons-by-groups
#| message: false
#| warning: false

# Create binary Z groups (high/low based on median split)
dag_data$Z_binary <- ifelse(dag_data$Z > median(dag_data$Z), "High Z", "Low Z")

# Calculate slopes and correlations for binary groups
binary_analysis <- dag_data %>%
  group_by(Z_binary) %>%
  summarise(
    n = n(),
    cor_XY = cor(X, Y),
    slope_XY = coef(lm(Y ~ X))[2],
    intercept_XY = coef(lm(Y ~ X))[1],
    mean_Z = mean(Z),
    mean_X = mean(X),
    mean_Y = mean(Y),
    .groups = 'drop'
  )

# Add confidence intervals for slopes
binary_analysis$slope_se <- NA
binary_analysis$slope_ci_lower <- NA
binary_analysis$slope_ci_upper <- NA

for (group in unique(dag_data$Z_binary)) {
  group_data <- dag_data[dag_data$Z_binary == group, ]
  model <- lm(Y ~ X, data = group_data)
  slope_se <- summary(model)$coefficients["X", "Std. Error"]
  slope_ci <- confint(model)["X", ]
  
  binary_analysis[binary_analysis$Z_binary == group, "slope_se"] <- slope_se
  binary_analysis[binary_analysis$Z_binary == group, "slope_ci_lower"] <- slope_ci[1]
  binary_analysis[binary_analysis$Z_binary == group, "slope_ci_upper"] <- slope_ci[2]
}

# Round for display
binary_analysis <- binary_analysis %>%
  mutate(across(where(is.numeric), ~ round(., 3)))

# Display binary group analysis
DT::datatable(binary_analysis,
              caption = "Binary Group Analysis for Simpson's Paradox Detection",
              options = list(pageLength = 10, scrollX = TRUE),
              colnames = c("Z Group", "N", "Correlation", "Slope", "Intercept", 
                          "Mean Z", "Mean X", "Mean Y", "Slope SE", "CI Lower", "CI Upper")) %>%
  DT::formatRound(columns = c("cor_XY", "slope_XY", "intercept_XY", "mean_Z", 
                              "mean_X", "mean_Y", "slope_se", "slope_ci_lower", "slope_ci_upper"), 
                  digits = 3)
```

### 12.4 Visual Detection of Simpson's Paradox

Let's create visualizations to clearly demonstrate whether Simpson's Paradox is present in our data:

```{r}
#| label: visualize-simpsons-paradox
#| fig-cap: "Visual Detection of Simpson's Paradox"
#| fig-subcap: 
#|   - "Overall relationship vs stratified relationships (quartiles)"
#|   - "Binary group analysis with separate regression lines"
#|   - "Slope comparison across different Z stratifications"
#| layout-ncol: 1

# Plot 1: Overall vs Stratified (Quartiles)
p1 <- ggplot(dag_data, aes(x = X, y = Y)) +
  # Overall regression line
  geom_smooth(method = "lm", formula = y ~ x, color = "red", linewidth = 1.5, 
              linetype = "dashed", se = FALSE) +
  # Stratified regression lines
  geom_smooth(aes(color = Z_quartile), method = "lm", formula = y ~ x, 
              linewidth = 1, se = FALSE) +
  # Points colored by quartile
  geom_point(aes(color = Z_quartile), alpha = 0.6) +
  scale_color_viridis_d(name = "Z Quartile") +
  labs(
    title = "Simpson's Paradox Analysis: Overall vs Stratified Relationships",
    subtitle = paste("Overall slope (red dashed):", round(overall_slope, 3),
                    "| Simpson's Status:", simpson_status),
    x = "X (Exposure)",
    y = "Y (Outcome)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Plot 2: Binary Group Analysis
p2 <- ggplot(dag_data, aes(x = X, y = Y)) +
  # Overall regression line
  geom_smooth(method = "lm", formula = y ~ x, color = "red", linewidth = 1.5, 
              linetype = "dashed", se = FALSE) +
  # Stratified regression lines for binary groups
  geom_smooth(aes(color = Z_binary), method = "lm", formula = y ~ x, 
              linewidth = 1.2, se = TRUE, alpha = 0.3) +
  # Points colored by binary group
  geom_point(aes(color = Z_binary), alpha = 0.6) +
  scale_color_manual(values = c("High Z" = "darkblue", "Low Z" = "darkgreen"),
                     name = "Z Group") +
  labs(
    title = "Binary Group Analysis for Simpson's Paradox",
    subtitle = "Red dashed line: Overall relationship | Colored lines: Group-specific relationships",
    x = "X (Exposure)",
    y = "Y (Outcome)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Plot 3: Slope Comparison
slope_comparison <- data.frame(
  Group = c("Overall", stratified_results$Z_quartile, binary_analysis$Z_binary),
  Slope = c(overall_slope, stratified_results$slope_XY, binary_analysis$slope_XY),
  Type = c("Overall", rep("Quartile", 4), rep("Binary", 2))
)

p3 <- ggplot(slope_comparison, aes(x = reorder(Group, Slope), y = Slope, fill = Type)) +
  geom_col(alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  geom_hline(yintercept = 0.4, linetype = "dotted", color = "red", alpha = 0.7) +
  scale_fill_manual(values = c("Overall" = "red", "Quartile" = "steelblue", "Binary" = "darkgreen")) +
  labs(
    title = "Slope Comparison Across Different Stratifications",
    subtitle = "Red dotted line shows true causal effect (0.4)",
    x = "Group",
    y = "Slope of X → Y"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display plots
p1
p2
p3
```

### 12.5 Formal Statistical Test for Simpson's Paradox

Let's conduct formal statistical tests to quantify the evidence for Simpson's Paradox:

```{r}
#| label: formal-test-simpsons-paradox
#| message: false
#| warning: false

# Function to test for significant differences in slopes
test_slope_differences <- function(overall_slope, overall_se, stratified_slopes, stratified_ses) {
  # Test if overall slope differs significantly from each stratified slope
  z_stats <- (overall_slope - stratified_slopes) / sqrt(overall_se^2 + stratified_ses^2)
  p_values <- 2 * (1 - pnorm(abs(z_stats)))
  
  return(list(z_stats = z_stats, p_values = p_values))
}

# Get standard errors for slopes
overall_model <- lm(Y ~ X, data = dag_data)
overall_se <- summary(overall_model)$coefficients["X", "Std. Error"]

# Get standard errors for stratified models
stratified_ses <- numeric(length(stratified_slopes))
for (i in 1:length(unique(dag_data$Z_quartile))) {
  quartile <- levels(dag_data$Z_quartile)[i]
  quartile_data <- dag_data[dag_data$Z_quartile == quartile, ]
  model <- lm(Y ~ X, data = quartile_data)
  stratified_ses[i] <- summary(model)$coefficients["X", "Std. Error"]
}

# Perform tests
slope_tests <- test_slope_differences(overall_slope, overall_se, stratified_slopes, stratified_ses)

# Create test results table
test_results <- data.frame(
  Comparison = paste("Overall vs", stratified_results$Z_quartile),
  Overall_Slope = rep(round(overall_slope, 3), length(stratified_slopes)),
  Stratified_Slope = round(stratified_slopes, 3),
  Difference = round(overall_slope - stratified_slopes, 3),
  Z_Statistic = round(slope_tests$z_stats, 3),
  P_Value = round(slope_tests$p_values, 3),
  Significant = ifelse(slope_tests$p_values < 0.05, "Yes", "No")
)

# Display test results
DT::datatable(test_results,
              caption = "Formal Statistical Tests for Slope Differences",
              options = list(pageLength = 10, scrollX = TRUE)) %>%
  DT::formatRound(columns = c("Overall_Slope", "Stratified_Slope", "Difference", 
                              "Z_Statistic", "P_Value"), digits = 3)
```

### 12.6 Magnitude of Simpson's Paradox Effect

Let's quantify the magnitude of the Simpson's Paradox effect if it exists:

```{r}
#| label: magnitude-simpsons-effect
#| message: false
#| warning: false

# Calculate Simpson's Paradox magnitude measures
simpson_magnitude <- data.frame(
  Measure = c(
    "Overall Slope",
    "Average Stratified Slope",
    "Absolute Difference",
    "Relative Difference (%)",
    "Direction Reversal",
    "Weighted Average Slope",
    "Simpson's Paradox Strength"
  ),
  Value = c(
    round(overall_slope, 3),
    round(mean(stratified_slopes), 3),
    round(abs(overall_slope - mean(stratified_slopes)), 3),
    round(100 * abs(overall_slope - mean(stratified_slopes)) / abs(mean(stratified_slopes)), 3),
    ifelse(sign(overall_slope) != sign(mean(stratified_slopes)), "Yes", "No"),
    # Weighted average by group size
    round(sum(stratified_results$slope_XY * stratified_results$n) / sum(stratified_results$n), 3),
    # Simpson's strength: how much the overall deviates from weighted average
    round(abs(overall_slope - sum(stratified_results$slope_XY * stratified_results$n) / sum(stratified_results$n)), 3)
  )
)

# Display magnitude analysis
DT::datatable(simpson_magnitude,
              caption = "Magnitude of Simpson's Paradox Effect",
              options = list(pageLength = 10, dom = 't'),
              colnames = c("Measure", "Value"))
```

### 12.7 Weighted vs Unweighted Analysis

Let's compare weighted and unweighted analyses to understand the role of group sizes:

```{r}
#| label: weighted-unweighted-analysis
#| message: false
#| warning: false

# Calculate weighted statistics
weighted_slope <- sum(stratified_results$slope_XY * stratified_results$n) / sum(stratified_results$n)
weighted_cor <- sum(stratified_results$cor_XY * stratified_results$n) / sum(stratified_results$n)

# Calculate unweighted statistics
unweighted_slope <- mean(stratified_results$slope_XY)
unweighted_cor <- mean(stratified_results$cor_XY)

# Group size analysis
group_size_analysis <- stratified_results %>%
  mutate(
    prop_of_total = round(n / sum(n), 3),
    slope_weight = round(slope_XY * prop_of_total, 3),
    cor_weight = round(cor_XY * prop_of_total, 3)
  ) %>%
  select(Z_quartile, n, prop_of_total, slope_XY, slope_weight, cor_XY, cor_weight)

# Add summary row
summary_row <- data.frame(
  Z_quartile = "Weighted Total",
  n = sum(group_size_analysis$n),
  prop_of_total = 1.000,
  slope_XY = round(weighted_slope, 3),
  slope_weight = round(sum(group_size_analysis$slope_weight), 3),
  cor_XY = round(weighted_cor, 3),
  cor_weight = round(sum(group_size_analysis$cor_weight), 3)
)

group_analysis_complete <- bind_rows(group_size_analysis, summary_row)

# Display weighted analysis
DT::datatable(group_analysis_complete,
              caption = "Weighted vs Unweighted Analysis by Group Size",
              options = list(pageLength = 10, scrollX = TRUE),
              colnames = c("Z Group", "N", "Proportion", "Slope", "Weighted Slope", 
                          "Correlation", "Weighted Correlation")) %>%
  DT::formatRound(columns = c("prop_of_total", "slope_XY", "slope_weight", 
                              "cor_XY", "cor_weight"), digits = 3)

# Summary comparison table
weighting_comparison <- data.frame(
  Analysis_Type = c("Overall (Marginal)", "Unweighted Average", "Weighted Average", 
                   "True Causal Effect"),
  Slope = c(round(overall_slope, 3), round(unweighted_slope, 3), 
           round(weighted_slope, 3), 0.400),
  Correlation = c(round(overall_cor, 3), round(unweighted_cor, 3), 
                 round(weighted_cor, 3), NA)
)

DT::datatable(weighting_comparison,
              caption = "Comparison of Different Analysis Approaches",
              options = list(pageLength = 10, dom = 't'),
              colnames = c("Analysis Type", "Slope Estimate", "Correlation"))
```

### 12.8 Conclusions from Simpson's Paradox Analysis

Based on our comprehensive analysis, here are the summarized findings about Simpson's Paradox in this synthetic dataset:

```{r}
#| label: simpsons-conclusions
#| message: false
#| warning: false

# Calculate key metrics for conclusions
direction_reversal <- sign(overall_slope) != sign(mean(stratified_slopes))
magnitude_difference <- abs(overall_slope - mean(stratified_slopes))
relative_magnitude <- 100 * magnitude_difference / abs(mean(stratified_slopes))

# Create conclusions summary
conclusions_data <- data.frame(
  Finding = c(
    "Simpson's Paradox Present?",
    "Direction of Overall Effect",
    "Direction of Stratified Effects",
    "Magnitude of Difference",
    "Relative Magnitude (%)",
    "Explanation",
    "Causal Interpretation",
    "Practical Implication"
  ),
  Result = c(
    simpson_status,
    ifelse(overall_slope > 0, "Positive", "Negative"),
    ifelse(mean(stratified_slopes) > 0, "Positive", "Negative"),
    paste(round(magnitude_difference, 3), "units"),
    paste(round(relative_magnitude, 1), "%"),
    "Confounding by Z creates different marginal vs conditional relationships",
    "Adjusted analysis reveals true causal effect",
    "Controlling for confounders is essential for causal inference"
  )
)

# Display conclusions
DT::datatable(conclusions_data,
              caption = "Summary of Simpson's Paradox Analysis Conclusions",
              options = list(pageLength = 10, dom = 't', scrollX = TRUE),
              colnames = c("Key Finding", "Result/Interpretation"))

# Create comprehensive summary table for key conclusions
paradox_present <- simpson_status != "No Simpson's Paradox"

key_conclusions <- data.frame(
  Category = c(
    "Simpson's Paradox Detection",
    "Simpson's Paradox Detection", 
    "Simpson's Paradox Detection",
    "Magnitude Analysis",
    "Magnitude Analysis",
    "Magnitude Analysis",
    "Causal Inference Implications",
    "Causal Inference Implications",
    "Causal Inference Implications"
  ),
  Finding = c(
    "Paradox Present?",
    "Status Classification",
    "Mechanistic Explanation",
    "Overall Slope",
    "Average Stratified Slope", 
    "Relative Difference",
    "Adjustment Set Validation",
    "Confounder Understanding",
    "Practical Importance"
  ),
  Result = c(
    ifelse(paradox_present, "✓ YES - Detected", "✗ NO - Not detected"),
    simpson_status,
    ifelse(paradox_present, 
           "Confounding creates paradoxical marginal vs conditional relationships",
           "Relationships remain consistent across Z strata"),
    round(overall_slope, 3),
    round(mean(stratified_slopes), 3),
    paste(round(relative_magnitude, 1), "%"),
    "Reinforces DAG-based conclusions about proper adjustment sets",
    "Validates understanding of confounders vs colliders", 
    "Demonstrates why causal structure consideration is essential"
  )
)

# Display the comprehensive conclusions table
DT::datatable(key_conclusions,
              caption = "Key Conclusions from Simpson's Paradox Analysis",
              options = list(
                pageLength = 15,
                ordering = FALSE,
                searching = FALSE,
                scrollX = TRUE
              ),
              class = 'cell-border stripe compact responsive',
              rownames = FALSE,
              width = "90%")
```

Our Simpson's Paradox analysis reveals important insights about the nature of confounding in causal inference. In this synthetic dataset, we observe **`r simpson_status`**, which demonstrates how the confounder Z creates different patterns when we examine the marginal (overall) versus conditional (stratified) relationships between X and Y.

The key insight is that Simpson's Paradox is not just a statistical curiosity—it's a fundamental illustration of why proper causal analysis requires careful consideration of confounding variables. In our fork structure DAG, the confounder Z affects both the exposure X and outcome Y, creating spurious associations that can mislead us about the true causal relationship.

Think of it this way: if we only looked at the overall relationship between X and Y, we might reach one conclusion. But when we properly account for Z by examining the relationship within Z groups, we see the true causal story. This is why randomized controlled trials—which balance confounders across treatment groups—are so valuable for causal inference.

The magnitude of the Simpson's Paradox effect in our data (**`r paste(round(relative_magnitude, 1), "%")`** relative difference) underscores the practical importance of identifying and controlling for key confounders in observational studies. This analysis reinforces our earlier findings about the critical role of proper adjustment in causal inference.




## Session Information for Reproducibility

```{r}
#| label: session-info
#| echo: false

# Session information for reproducibility
sessionInfo()

```





