---
title: "DAG Analysis with Synthetic Data - Fork Structure"
author: "Your Name"
format: 
  html:
    toc: true
    toc-float: true
    page-layout: article
    embed-resources: true
    code-fold: true
    code-summary: "Show the code"
    code-tools: true
    code-overflow: wrap
    code-block-bg: "#FAEBD7"
    code-block-border-left: "#31BAE9"
    code-link: true
    fig-width: 8
    fig-height: 6
    fig-align: center
    html-math-method: katex
---

```{r}
#| label: init-setup
#| include: false
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE)

# Load required packages
library(tidyverse)    # For dplyr, ggplot, and friends
library(ggdag)        # For plotting DAGs
library(dagitty)      # For working with DAG logic
library(DiagrammeR)   # For complete control of the layout
library(knitr)        # For controlling rendering
library(kableExtra)   # For tables summarizing results
library(DT)           # For rendering interactive tables
library(rethinking)   # For causal inference models 
library(broom)        # For tidying model output
```

## 1. Introduction: Testing a Fork Structure DAG with Synthetic Data

This document demonstrates how to test a simple fork structure (confounder) DAG using synthetic data. In our DAG, we have:

- **X**: Exposure variable (e.g., coffee consumption)
- **Y**: Outcome variable (e.g., heart disease risk)
- **Z**: Confounder variable (e.g., age)

Our causal model specifies that:
- Z affects both X and Y (Z → X and Z → Y)
- X affects Y (X → Y)

This structure creates a classic confounding scenario where failing to account for Z will lead to biased estimates of the effect of X on Y.

## 2. Creating Synthetic Data

First, let's create synthetic data that follows our DAG structure. We'll set the true causal relationships with specific coefficients:

```{r}
#| label: create-synthetic-data
#| message: false
#| warning: false

# Set seed for reproducibility
set.seed(42)

# Sample size
n <- 1000

# Generate synthetic data following the DAG structure
# Z → X, Z → Y, and X → Y

# First, generate the confounder Z (e.g., age)
Z <- rnorm(n, mean = 50, sd = 10)

# Next, generate X as influenced by Z (e.g., coffee consumption affected by age)
# We add some random noise to represent other factors affecting X
X <- 2 + 0.08 * Z + rnorm(n, mean = 0, sd = 1)

# Finally, generate Y as influenced by both Z and X
# The true direct effect of X on Y is 0.3
# The true effect of Z on Y is 0.1
Y <- 5 + 0.3 * X + 0.1 * Z + rnorm(n, mean = 0, sd = 1.5)

# Create a data frame
dag_data <- data.frame(X = X, Y = Y, Z = Z)

# Quick summary of the data
summary(dag_data)
```

Our synthetic data now follows these true causal relationships:
- Z → X with coefficient 0.08 (1 unit increase in Z causes 0.08 unit increase in X)
- Z → Y with coefficient 0.1 (1 unit increase in Z causes 0.1 unit increase in Y)
- X → Y with coefficient 0.3 (1 unit increase in X causes 0.3 unit increase in Y)

Let's visualize the distributions and relationships in our synthetic data:

```{r}
#| label: visualize-data-distributions
#| fig-cap: "Distributions and relationships in our synthetic data"
#| fig-subcap: 
#|   - "Distributions of X, Y, and Z"
#|   - "Scatterplot of X vs Y"
#|   - "Scatterplot of Z vs X"
#|   - "Scatterplot of Z vs Y"
#| layout-ncol: 2

# Distributions of variables
dag_data %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value)) +
  geom_histogram(fill = "steelblue", alpha = 0.7, bins = 30) +
  facet_wrap(~ Variable, scales = "free") +
  theme_minimal() +
  ggtitle("Distributions of X, Y, and Z")

# X vs Y scatterplot
ggplot(dag_data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.3, color = "dodgerblue") +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  theme_minimal() +
  ggtitle("Relationship between X and Y")

# Z vs X scatterplot
ggplot(dag_data, aes(x = Z, y = X)) +
  geom_point(alpha = 0.3, color = "darkgreen") +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  theme_minimal() +
  ggtitle("Relationship between Z and X")

# Z vs Y scatterplot
ggplot(dag_data, aes(x = Z, y = Y)) +
  geom_point(alpha = 0.3, color = "purple") +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  theme_minimal() +
  ggtitle("Relationship between Z and Y")
```

## 3. Recreating the DAG for Reference

Let's recreate the DAG using DiagrammeR and ggdag for reference:

```{r}
#| label: visualize-dag-structure
#| message: false
#| warning: false
#| fig-cap: "DAG visualization showing the fork structure"
#| fig-subcap: 
#|   - "DiagrammeR Visualization"
#|   - "ggdag Visualization"
#| layout-ncol: 1

# Visualize DAG with DiagrammeR
grViz("
  digraph DAG {
    # Graph settings
    graph [layout=neato, margin=\"0.0, 0.0, 0.0, 0.0\"]
    
    # Add a title
    labelloc=\"t\"
    label=\"Causal Fork Structure DAG\\n   \"
    fontsize=16
    
    # Node settings
    node [shape=plaintext, fontsize=16, fontname=\"Arial\"]
    
    # Edge settings
    edge [penwidth=1.50, color=\"darkblue\", arrowsize=1.00]
    
    # Nodes with exact coordinates
    X [label=\"X (Exposure)\", pos=\"1.0, 1.0!\", fontcolor=\"dodgerblue\"]
    Y [label=\"Y (Outcome)\", pos=\"3.0, 1.0!\", fontcolor=\"dodgerblue\"]
    Z [label=\"Z (Confounder)\", pos=\"2.0,3.0!\", fontcolor=\"red\"]
    
    # Edges with true coefficients from our synthetic data
    X -> Y [label=\"0.3\"]
    Z -> X [label=\"0.08\"]
    Z -> Y [label=\"0.1\"]
    
    # Caption
    Caption [shape=plaintext, label=\"Figure 1: Z represents a classic confounding structure\", 
             fontsize=10, pos=\"2,0.0!\"]
  }
")

# Define the DAG using ggdag
dag <- dagify(
  Y ~ X + Z,
  X ~ Z,
  exposure = "X",
  outcome = "Y",
  labels = c("X" = "X (Exposure)", 
             "Y" = "Y (Outcome)", 
             "Z" = "Z (Confounder)")
)

# Set coordinates for nice visualization
coordinates(dag) <- list(
  x = c(X = 1, Y = 3, Z = 2),
  y = c(X = 1, Y = 1, Z = 3)
)

# Create nice visualization with ggdag
ggdag(dag, edge_type = "link") + 
  geom_dag_point(color = "lightblue", size = 14, alpha = 0.7) +
  geom_dag_text(color = "black") +
  geom_dag_edges(edge_colour = "blue", edge_width = 1.0, arrow_size = 0.6) +
  theme_dag() +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("DAG: X → Y with Z as confounder")
```

## 4. Testing Causal Relationships

Now let's test the causal relationships in our DAG by comparing models with and without adjusting for the confounder Z.

### 4.1 Unadjusted Model (Biased Estimate)

First, we'll fit a model that ignores the confounder Z:

```{r}
#| label: fit-unadjusted-model
#| message: false

# Fit unadjusted model (ignoring the confounder Z)
model_unadjusted <- lm(Y ~ X, data = dag_data)

# Display model summary
summary(model_unadjusted)

# Extract the coefficient for X
coef_unadjusted <- coef(model_unadjusted)["X"]
```

### 4.2 Adjusted Model (Correcting for Confounding)

Now, let's fit a model that adjusts for the confounder Z:

```{r}
#| label: fit-adjusted-model
#| message: false

# Fit adjusted model (accounting for confounder Z)
model_adjusted <- lm(Y ~ X + Z, data = dag_data)

# Display model summary
summary(model_adjusted)

# Extract the coefficient for X
coef_adjusted <- coef(model_adjusted)["X"]
```

### 4.3 Comparing Model Results

Let's compare the estimated effect of X on Y from both models:

```{r}
#| label: compare-model-results
#| message: false

# True effect of X on Y (from our data generation process)
true_effect <- 0.3

# Create a comparison table
comparison_df <- data.frame(
  Model = c("True Causal Effect", "Unadjusted Model (Ignores Z)", "Adjusted Model (Controls for Z)"),
  Coefficient = c(true_effect, coef_unadjusted, coef_adjusted),
  Error = c(0, coef_unadjusted - true_effect, coef_adjusted - true_effect),
  BiasPercent = c(0, 100 * (coef_unadjusted - true_effect) / true_effect, 
                  100 * (coef_adjusted - true_effect) / true_effect)
)

# Format for display
comparison_df$Coefficient <- round(comparison_df$Coefficient, 4)
comparison_df$Error <- round(comparison_df$Error, 4)
comparison_df$BiasPercent <- round(comparison_df$BiasPercent, 2)

# Display as a table
kable(comparison_df, 
      col.names = c("Model", "Coefficient of X on Y", "Absolute Error", "Bias (%)"),
      caption = "Comparison of estimated effects of X on Y") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

### 4.4 Testing for Significant Differences Between Models

Now, let's test whether the difference between the unadjusted and adjusted models is statistically significant, and whether our adjusted model recovers the true causal effect:

```{r}
#| label: test-model-significance
#| message: false

# Compare models using anova (valid since one model is nested in the other)
model_comparison <- anova(model_unadjusted, model_adjusted)

# Test if unadjusted coefficient differs from true effect
unadj_z_stat <- (coef_unadjusted - true_effect) / summary(model_unadjusted)$coefficients["X", "Std. Error"]
unadj_p_value <- 2 * (1 - pnorm(abs(unadj_z_stat)))

# Test if adjusted coefficient differs from true effect
adj_z_stat <- (coef_adjusted - true_effect) / summary(model_adjusted)$coefficients["X", "Std. Error"]
adj_p_value <- 2 * (1 - pnorm(abs(adj_z_stat)))

# Create a data frame for the results
significance_df <- data.frame(
  Comparison = c(
    "Unadjusted vs. Adjusted Model",
    "Unadjusted Model vs. True Effect",
    "Adjusted Model vs. True Effect"
  ),
  
  Test = c(
    "F-test (ANOVA)",
    "Z-test (coefficient vs. true effect)",
    "Z-test (coefficient vs. true effect)"
  ),
  
  Statistic = c(
    round(model_comparison$F[2], 3),
    round(unadj_z_stat, 3),
    round(adj_z_stat, 3)
  ),
  
  PValue = c(
    round(model_comparison$`Pr(>F)`[2], 4),
    round(unadj_p_value, 4),
    round(adj_p_value, 4)
  ),
  
  Conclusion = c(
    ifelse(model_comparison$`Pr(>F)`[2] < 0.05, 
           "Models are significantly different", 
           "No significant difference between models"),
    
    ifelse(unadj_p_value < 0.05,
           "Unadjusted estimate is significantly different from true effect",
           "Unadjusted estimate is not significantly different from true effect"),
    
    ifelse(adj_p_value < 0.05,
           "Adjusted estimate is significantly different from true effect",
           "Adjusted estimate is not significantly different from true effect")
  )
)

# Display as a table
kable(significance_df, 
      col.names = c("Comparison", "Test", "Statistic", "P-value", "Conclusion"),
      caption = "Statistical tests for model comparisons") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

These tests confirm that:

1. The unadjusted and adjusted models are significantly different from each other (ANOVA F-test)
2. The unadjusted model's estimate is significantly different from the true causal effect (confirming bias)
3. The adjusted model's estimate is not significantly different from the true causal effect (confirming it recovers the true relationship)

This statistical evidence supports our claim that failing to control for confounder Z leads to a significantly biased estimate of the effect of X on Y.

## 5. Testing Implied Conditional Independences 

The fork structure DAG implies a specific conditional independence: **X and Y should be conditionally independent given Z** if the only association between X and Y is through Z. However, since we have a direct effect of X on Y, we should see that X and Y remain associated even after conditioning on Z.

Let's test this using partial correlations:

```{r}
#| label: test-conditional-independence
#| message: false

# Function to calculate partial correlation
partial_cor <- function(x, y, z, data) {
  model_x <- lm(as.formula(paste(x, "~", z)), data = data)
  model_y <- lm(as.formula(paste(y, "~", z)), data = data)
  
  residuals_x <- residuals(model_x)
  residuals_y <- residuals(model_y)
  
  cor(residuals_x, residuals_y)
}

# Calculate simple correlation between X and Y
simple_cor_XY <- cor(dag_data$X, dag_data$Y)

# Calculate partial correlation between X and Y, controlling for Z
partial_cor_XY_Z <- partial_cor("X", "Y", "Z", dag_data)

# Prepare results for display
cor_tests <- data.frame(
  Test = c("Simple correlation between X and Y", 
           "Partial correlation between X and Y, controlling for Z"),
  Correlation = c(simple_cor_XY, partial_cor_XY_Z)
)

# Format for display
cor_tests$Correlation <- round(cor_tests$Correlation, 4)

# Display as a table
kable(cor_tests, 
      col.names = c("Test", "Correlation"),
      caption = "Testing conditional independences") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

The results confirm our understanding of the DAG structure:

1. The simple correlation between X and Y is substantial, reflecting both the direct effect of X on Y and the indirect association through Z
2. The partial correlation between X and Y controlling for Z is reduced but still significant, confirming the direct causal relationship between X and Y

## 6. Path Analysis in Our DAG

Our DAG has two paths from X to Y:
1. The direct path: X → Y (causal effect)
2. The backdoor path: X ← Z → Y (confounding)

Let's visualize these paths:

```{r}
#| label: visualize-dag-paths
#| message: false
#| warning: false
#| fig-cap: "Path analysis in our DAG"

# Use ggdag to visualize paths
ggdag_paths(dag, from = "X", to = "Y", shadow = TRUE) +
  theme_dag() +
  ggtitle("All paths from X to Y")
```

Now, let's use ggdag to highlight the appropriate adjustment set:

```{r}
#| label: visualize-adjustment-set
#| message: false
#| warning: false
#| fig-cap: "Adjustment set for estimating the causal effect of X on Y"

# Visualize the adjustment set
ggdag_adjustment_set(dag, exposure = "X", outcome = "Y", shadow = TRUE) +
  theme_dag() +
  ggtitle("Adjustment set to estimate causal effect of X on Y")
```

## 7. Examining the Backdoor Criterion

The backdoor criterion states that to identify the causal effect of X on Y, we need to block all backdoor paths between X and Y. In our case, there's one backdoor path: X ← Z → Y.

Let's see how controlling for Z changes the estimated relationship between X and Y by visualizing this process:

```{r}
#| label: visualize-backdoor-criterion
#| fig-cap: "Visualizing how controlling for Z removes confounding"
#| fig-subcap: 
#|   - "Relationship between X and Y (unadjusted)"
#|   - "Relationship between Z and X (confounder → exposure)"
#|   - "Relationship between Z and Y (confounder → outcome)"
#|   - "Relationship between X and Y after adjusting for Z"
#| layout-ncol: 2

# 1. X-Y relationship (unadjusted)
p1 <- ggplot(dag_data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.3, color = "dodgerblue") +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  theme_minimal() +
  annotate("text", x = min(dag_data$X) + 0.2*(max(dag_data$X)-min(dag_data$X)), 
           y = max(dag_data$Y) - 0.1*(max(dag_data$Y)-min(dag_data$Y)), 
           label = paste("Slope =", round(coef(lm(Y ~ X, dag_data))[2], 3)),
           hjust = 0) +
  ggtitle("Unadjusted X-Y relationship")

# 2. Z-X relationship (correctly labeled with Z as predictor)
p2 <- ggplot(dag_data, aes(x = Z, y = X)) +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  geom_point(alpha = 0.3, color = "darkgreen") +
  theme_minimal() +
  annotate("text", x = min(dag_data$Z) + 0.2*(max(dag_data$Z)-min(dag_data$Z)), 
           y = max(dag_data$X) - 0.1*(max(dag_data$X)-min(dag_data$X)), 
           label = paste("Slope =", round(coef(lm(X ~ Z, dag_data))[2], 3)),
           hjust = 0) +
  ggtitle("Z → X relationship (confounder affects exposure)")

# 3. Z-Y relationship (correctly labeled with Z as predictor)
p3 <- ggplot(dag_data, aes(x = Z, y = Y)) +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  geom_point(alpha = 0.3, color = "purple") +
  theme_minimal() +
  annotate("text", x = min(dag_data$Z) + 0.2*(max(dag_data$Z)-min(dag_data$Z)), 
           y = max(dag_data$Y) - 0.1*(max(dag_data$Y)-min(dag_data$Y)), 
           label = paste("Slope =", round(coef(lm(Y ~ Z, dag_data))[2], 3)),
           hjust = 0) +
  ggtitle("Z → Y relationship (confounder affects outcome)")

# 4. X-Y relationship (adjusted for Z) - using residuals
# Create residuals
x_resid <- residuals(lm(X ~ Z, data = dag_data))
y_resid <- residuals(lm(Y ~ Z, data = dag_data))
resid_data <- data.frame(x_resid = x_resid, y_resid = y_resid)

# Plot relationship between residuals
p4 <- ggplot(resid_data, aes(x = x_resid, y = y_resid)) +
  geom_smooth(method = "lm", formula = y ~ x, color = "darkred") +
  geom_point(alpha = 0.3, color = "orange") +
  theme_minimal() +
  annotate("text", x = min(resid_data$x_resid) + 0.2*(max(resid_data$x_resid)-min(resid_data$x_resid)), 
           y = max(resid_data$y_resid) - 0.1*(max(resid_data$y_resid)-min(resid_data$y_resid)), 
           label = paste("Slope =", round(coef(lm(y_resid ~ x_resid))[2], 3)),
           hjust = 0) +
  xlab("X (residuals after controlling for Z)") +
  ylab("Y (residuals after controlling for Z)") +
  ggtitle("X → Y relationship after removing Z's effect")

# Display all plots
p1
p2
p3
p4
```

The visualizations above demonstrate the process of controlling for a confounder:

1. The unadjusted X-Y relationship (top left) shows a strong association that includes both the causal effect and confounding
2. The Z-X relationship (top right) shows how the confounder affects our exposure variable
3. The Z-Y relationship (bottom left) shows how the confounder affects our outcome variable
4. The adjusted X-Y relationship (bottom right), after removing Z's effect from both X and Y, shows the true causal relationship

## 8. D-Separation Analysis

D-separation is a criterion for determining whether two variables in a DAG are conditionally independent given a set of other variables. Let's test the d-separation claims from our DAG:

```{r}
#| label: test-d-separation
#| message: false

# Test d-separation claims in our DAG
dseparation_results <- data.frame(
  Claim = c(
    "X and Y are not d-separated (without conditioning)",
    "X and Y are not d-separated given Z"
  ),
  Result = c(
    !dseparated(dag, "X", "Y", c()),
    !dseparated(dag, "X", "Y", c("Z"))
  ),
  Explanation = c(
    "X and Y are associated through both the direct path and the backdoor path",
    "Even after blocking the backdoor path, X and Y remain associated through the direct path"
  )
)

# Display as a table
kable(dseparation_results, 
      col.names = c("D-Separation Claim", "Confirmed?", "Explanation"),
      caption = "Testing d-separation claims in our DAG") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

These results confirm our understanding of the DAG structure:

1. X and Y are not d-separated (without conditioning) because they have both a direct connection and a backdoor path through Z
2. X and Y remain associated even after conditioning on Z because of the direct causal path from X to Y

## 9. Sensitivity Analysis: What if Z is Unmeasured?

In real-world scenarios, we might not be able to measure all confounders. Let's conduct a sensitivity analysis to understand the impact of unmeasured confounding:

```{r}
#| label: conduct-sensitivity-analysis
#| message: false
#| warning: false

# Function to simulate unmeasured confounding
simulate_unmeasured_z <- function(cor_xz, cor_zy, n = 1000) {
  # Create a new dataset without Z
  z_unmeasured <- rnorm(n)
  
  # Create X correlated with unmeasured Z
  x_unmeasured <- cor_xz * z_unmeasured + sqrt(1 - cor_xz^2) * rnorm(n)
  
  # Create Y correlated with unmeasured Z and with a direct effect from X (set to 0.3)
  y_unmeasured <- 0.3 * x_unmeasured + cor_zy * z_unmeasured + 
                  sqrt(1 - 0.3^2 - cor_zy^2 - 2*0.3*cor_xz*cor_zy) * rnorm(n)
  
  # Create dataset
  data.frame(X = x_unmeasured, Y = y_unmeasured)
}

# Create a range of confounding strengths
z_effects <- seq(0, 0.5, by = 0.1)
results <- data.frame()

# For each confounding strength, calculate the bias
for (z_effect in z_effects) {
  # Simulate data with unmeasured confounding
  sim_data <- simulate_unmeasured_z(cor_xz = z_effect, cor_zy = z_effect)
  
  # Fit model without ability to adjust for Z
  model <- lm(Y ~ X, data = sim_data)
  
  # Store results
  results <- rbind(results, data.frame(
    ConfounderStrength = z_effect,
    EstimatedEffect = coef(model)["X"],
    TrueEffect = 0.3,
    Bias = coef(model)["X"] - 0.3,
    BiasPercent = 100 * (coef(model)["X"] - 0.3) / 0.3
  ))
}

# Plot the results
ggplot(results, aes(x = ConfounderStrength, y = EstimatedEffect)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "blue", size = 3) +
  geom_hline(yintercept = 0.3, linetype = "dashed", color = "red") +
  labs(
    title = "Impact of Unmeasured Confounding on Effect Estimates",
    subtitle = "Red dashed line shows true causal effect (0.3)",
    x = "Confounder Strength (correlation between Z and X/Y)",
    y = "Estimated Effect of X on Y"
  ) +
  theme_minimal()
```

This sensitivity analysis shows how unmeasured confounding can bias our estimates of the causal effect. As the strength of confounding increases (stronger correlation between Z and both X and Y), our estimate of the effect of X on Y becomes increasingly biased.

## 10. Beyond Simple Regression: Causal Simulation and Estimation

Let's explore a more advanced approach using the `rethinking` package, which is especially good for causal inference:

```{r}
#| label: fit-bayesian-model
#| message: false
#| warning: false

# Standardize the variables for better fitting
dag_data_std <- dag_data %>%
  mutate(across(everything(), scale))

# Fit a Bayesian causal model for the adjusted effect
m_adjusted <- quap(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + bX * X + bZ * Z,
    a ~ dnorm(0, 1),
    bX ~ dnorm(0, 1),
    bZ ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = dag_data_std
)

# Extract the posterior distribution
post <- extract.samples(m_adjusted)

# Summarize the posterior for the causal effect of X on Y
causal_effect_summary <- data.frame(
  Parameter = "Effect of X on Y",
  Mean = mean(post$bX),
  SD = sd(post$bX),
  `2.5%` = quantile(post$bX, 0.025),
  `50%` = median(post$bX),
  `97.5%` = quantile(post$bX, 0.975)
)

# Display the causal effect estimate
kable(causal_effect_summary, 
      caption = "Bayesian estimate of the causal effect of X on Y (adjusted for Z)",
      digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# Plot the posterior distribution of the causal effect
ggplot(data.frame(bX = post$bX), aes(x = bX)) +
  geom_density(fill = "skyblue", alpha = 0.7) +
  geom_vline(xintercept = mean(post$bX), color = "darkblue", linetype = "dashed") +
  labs(
    title = "Posterior Distribution of the Causal Effect of X on Y",
    subtitle = "After adjusting for confounder Z",
    x = "Effect Size (Standardized)",
    y = "Density"
  ) +
  theme_minimal()
```

## 11. Counterfactual Analysis: "What If" Scenarios

Causal inference allows us to answer counterfactual questions. Let's explore what would happen to Y if we intervened to change X, while holding Z constant:


```{r}
#| label: counterfactual-analysis
#| message: false
#| warning: false

# Function to predict Y based on do(X = x)
predict_counterfactual <- function(x_values, fixed_z = 50) {
  # Use the coefficients from our adjusted model
  intercept <- coef(model_adjusted)[1]
  x_coef <- coef(model_adjusted)[2]
  z_coef <- coef(model_adjusted)[3]
  
  # Predict Y for different values of X, holding Z constant
  y_pred <- intercept + x_coef * x_values + z_coef * fixed_z
  return(y_pred)
}

# Create a range of X values
x_range <- seq(min(dag_data$X), max(dag_data$X), length.out = 100)

# Predict Y for different values of X, holding Z constant at its mean
z_mean <- mean(dag_data$Z)
y_pred <- predict_counterfactual(x_range, fixed_z = z_mean)

# Create a data frame for plotting
counterfactual_df <- data.frame(X = x_range, Y = y_pred)

# Plot the counterfactual prediction
ggplot() +
  # Add actual data points
  geom_point(data = dag_data, aes(x = X, y = Y), alpha = 0.2, color = "gray") +
  # Add counterfactual prediction
  geom_line(data = counterfactual_df, aes(x = X, y = Y), 
            color = "red", size = 1.2) +
  labs(
    title = "Counterfactual Prediction: What if we change X?",
    subtitle = paste("Holding Z constant at its mean:", round(z_mean, 2)),
    x = "X (Exposure)",
    y = "Y (Outcome)"
  ) +
  theme_minimal()
```

The red line represents the causal effect of X on Y, isolated from confounding by holding Z constant. This is the relationship we would observe if we could experimentally manipulate X while keeping Z fixed.

To understand how the counterfactual prediction changes for different values of Z, let's create a set of predictions for various Z values:

```{r}
#| label: counterfactual-multiple-z
#| message: false
#| warning: false

# Create predictions for different Z values
z_values <- quantile(dag_data$Z, probs = c(0.1, 0.25, 0.5, 0.75, 0.9))
predictions_list <- list()

for (z_val in z_values) {
  y_pred <- predict_counterfactual(x_range, fixed_z = z_val)
  predictions_list[[as.character(round(z_val, 1))]] <- y_pred
}

# Convert to a data frame for ggplot
predictions_df <- data.frame(X = x_range)
for (z_val in names(predictions_list)) {
  predictions_df[[paste0("Z_", z_val)]] <- predictions_list[[z_val]]
}

# Reshape for plotting
predictions_long <- predictions_df %>%
  pivot_longer(cols = starts_with("Z_"), 
               names_to = "Z_value", 
               values_to = "Y_pred") %>%
  mutate(Z_value = gsub("Z_", "", Z_value))

# Plot the counterfactual predictions for different Z values
ggplot() +
  # Add actual data points
  geom_point(data = dag_data, aes(x = X, y = Y), alpha = 0.1, color = "gray") +
  # Add counterfactual predictions
  geom_line(data = predictions_long, 
            aes(x = X, y = Y_pred, color = Z_value, group = Z_value),
            size = 1.0) +
  scale_color_viridis_d(name = "Z Value\n(Age)") +
  labs(
    title = "Counterfactual Predictions for Different Z Values",
    subtitle = "Each line shows the causal effect of X on Y for a specific Z value",
    x = "X (Exposure, e.g., Coffee Consumption)",
    y = "Y (Outcome, e.g., Heart Disease Risk)"
  ) +
  theme_minimal()
```

This visualization demonstrates an important feature of causal inference: we can predict the effect of interventions under different scenarios or for different subgroups. Each line shows how Y would respond to changes in X for a specific value of Z. This type of analysis can be valuable for personalized predictions or targeted interventions.

## 12. Practical Implications and Conclusions

Our analysis of the fork structure DAG with synthetic data demonstrates several key principles of causal inference:

### 12.1 Summary of Key Findings

1. **Importance of adjusting for confounders**: We showed that failing to control for confounder Z leads to a biased estimate of the effect of X on Y. Our statistical tests confirmed that the unadjusted estimate is significantly different from the true causal effect, while the adjusted estimate successfully recovers it.

2. **Backdoor criterion in action**: By conditioning on Z, we blocked the backdoor path X ← Z → Y, allowing us to estimate the true causal effect of X on Y. This was visualized through our residuals analysis, which demonstrated how removing Z's influence isolates the direct X → Y relationship.

3. **D-separation properties**: We verified that X and Y remain associated even after conditioning on Z, confirming the direct causal relationship between X and Y.

4. **Quantifying unmeasured confounding**: Our sensitivity analysis demonstrated how unmeasured confounding can bias causal estimates, emphasizing the importance of identifying and measuring key confounders. The bias increased progressively with the strength of confounding.

5. **Counterfactual prediction**: We used our causal model to predict what would happen to Y if we intervened to change X while holding Z constant—a key capability of causal inference that goes beyond standard predictive modeling.

### 12.2 Real-World Application Context

In a real-world setting like our coffee consumption and heart disease example:

- **X (coffee consumption)** might be measured in cups per day
- **Y (heart disease risk)** might be measured as a risk score or incidence rate
- **Z (age)** would be measured in years

Our analysis helps us understand whether coffee consumption truly affects heart disease risk, or whether the observed association is partially or entirely due to the confounding effect of age.

For example, without adjusting for age, we might conclude that drinking more coffee increases heart disease risk, when in reality older people simply tend to both drink more coffee and have higher heart disease risk. The adjusted analysis reveals the true causal relationship, which could potentially show that coffee has a smaller effect or even a protective effect once age is properly accounted for.

### 12.3 Methodological Insights

Our analysis demonstrated several powerful techniques for causal inference:

1. **Multiple modeling approaches**: We used both frequentist (linear regression with adjustment) and Bayesian (quap) methods, showing how different statistical frameworks can be applied to causal questions.

2. **Visualization of confounding**: Our residual plots provided an intuitive way to understand how controlling for a confounder changes the estimated relationship between X and Y.

3. **Sensitivity analysis**: We quantified how unmeasured confounding could affect our conclusions, an important step for assessing the robustness of causal claims.

4. **Counterfactual prediction**: We showed how to generate predictions for hypothetical interventions, a unique capability of causal models compared to purely predictive models.

### 12.4 Limitations and Extensions

This simple fork structure is just one of many possible causal patterns. Real-world causal relationships often involve:

- Multiple confounders
- Mediator variables (on the causal path between X and Y)
- Collider variables (affected by both X and Y)
- Instrumental variables (affecting X but not directly affecting Y)
- Cyclic relationships (feedback loops)

More complex DAGs would require more sophisticated analysis techniques, but the principles demonstrated here form the foundation of causal inference with DAGs.

### 12.5 Recommendations for Empirical Research

Based on our analysis, we recommend the following practices for causal inference in empirical research:

1. **Draw out your causal model**: Explicitly representing causal assumptions in a DAG forces clarity of thinking and helps identify potential sources of bias.

2. **Identify minimal sufficient adjustment sets**: Use tools like `dagitty` to determine which variables need to be controlled for to estimate causal effects.

3. **Test implied conditional independencies**: Use your DAG to derive testable predictions about conditional independence relationships, and check these against your data.

4. **Conduct sensitivity analyses**: Assess how robust your causal conclusions are to potential unmeasured confounding or model misspecification.

5. **Think counterfactually**: Frame causal questions in terms of "what would happen if we intervened" rather than just associations.

By following these principles and using the methods demonstrated in this analysis, researchers can move beyond correlation to make stronger causal claims from observational data.



## Session Information for Reproducibility

```{r}
#| label: session-info
#| echo: false

# Session information for reproducibility
# sessionInfo()

```

  
